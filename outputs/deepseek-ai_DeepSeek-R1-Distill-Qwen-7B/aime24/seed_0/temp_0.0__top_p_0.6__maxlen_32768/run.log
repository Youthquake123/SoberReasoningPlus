---- 2025-08-22T08:49:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 7 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 08:50:08.163670519 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:51:19 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 08:52:23.335416442 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:53:08 [__init__.py:244] Automatically detected platform cuda.
[I822 08:53:20.034449693 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037203470 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037209454 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037212036 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037213494 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037224038 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:53:20.037938791 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 08:54:19 [__init__.py:244] Automatically detected platform cuda.
[I822 08:54:42.973692399 TCPStore.cpp:274] [c10d - debug] The server has started on port = 47257.
[I822 08:54:42.973709875 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 08:54:42.973820006 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.975734377 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=124, addr=[localhost]:43664, remote=[localhost]:47257).
[I822 08:54:42.978757329 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.234160662 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.234352399 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.234780362 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.237838156 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.236451443 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43684, remote=[localhost]:47257).
[I822 08:54:42.239253982 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.236227399 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43680, remote=[localhost]:47257).
[I822 08:54:42.239330601 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.236845455 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43696, remote=[localhost]:47257).
[I822 08:54:42.239971273 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.239264981 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43706, remote=[localhost]:47257).
[I822 08:54:42.242516285 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.244017984 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.245384542 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43720, remote=[localhost]:47257).
[I822 08:54:42.247843793 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.283598783 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 7, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.283601660 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 7, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.283600730 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 7, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.283612048 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.283612925 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.283613888 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.283608957 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 7, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.283621832 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.283653141 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 7, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.283662106 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:42.288554702 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 7, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 08:54:42.288557022 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 7, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 08:54:42.288560252 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 7, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 08:54:42.288564092 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:42.288564150 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:42.288563873 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 7, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 08:54:42.288566786 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:42.288566735 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 7, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 08:54:42.288571157 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:42.288575485 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.303114678 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 47257).
[I822 08:54:42.304649844 socket.cpp:946] [c10d] The client socket has connected to [localhost]:47257 on SocketImpl(fd=116, addr=[localhost]:43730, remote=[localhost]:47257).
[I822 08:54:42.307522404 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:47257
[I822 08:54:42.307976570 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 7, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.307987249 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:54:42.308363443 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 7, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 08:54:42.308371880 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 08:54:42.317775703 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 7, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 08:54:42.317789166 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:54:42.318105968 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 7, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 08:54:42.318113890 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:54:42.457743853 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 7, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 08:54:42.457746864 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 7, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 08:54:42.457750486 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 7, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 08:54:42.457754006 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:42.457754276 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:42.457752803 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 7, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 08:54:42.457753280 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 7, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 08:54:42.457755576 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 7, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 08:54:42.457758069 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:42.457760817 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:54:42.457761112 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:42.457762886 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:54:42.457774818 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 7, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 08:54:42.457781975 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:117:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755852883::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755852883::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=1 pid=113)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=6 pid=118)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=2 pid=114)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=5 pid=117)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=4 pid=116)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=3 pid=115)[0;0m Custom allreduce is disabled due to an unsupported world size: 7. Supported world sizes: [2, 4, 6, 8]. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[rank0]:[I822 08:54:44.772863813 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 08:54:44.772878646 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:54:44.772899482 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank1]:[I822 08:54:44.772909882 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:44.772921914 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank2]:[I822 08:54:44.772933207 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:44.772955042 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank3]:[I822 08:54:44.772966372 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:54:44.772985729 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank4]:[I822 08:54:44.772997697 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:44.773007331 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank6]:[I822 08:54:44.773019510 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:54:44.773018297 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank5]:[I822 08:54:44.773026952 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:54:44.774139016 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank3]:[I822 08:54:44.774143240 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank0]:[I822 08:54:44.774149542 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:44.774151011 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:54:44.774149295 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank4]:[I822 08:54:44.774151211 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank1]:[I822 08:54:44.774154540 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank5]:[I822 08:54:44.774158253 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:54:44.774158583 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:44.774156761 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank1]:[I822 08:54:44.774161801 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:44.774159373 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank2]:[I822 08:54:44.774164210 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:44.774166703 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 08:54:44.775114593 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 6] ProcessGroupNCCL initialization options: size: 7, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank6]:[I822 08:54:44.775124453 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 08:54:44.775140472 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 5] ProcessGroupNCCL initialization options: size: 7, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank5]:[I822 08:54:44.775154102 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 08:54:44.775167022 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 4] ProcessGroupNCCL initialization options: size: 7, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank4]:[I822 08:54:44.775179869 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 08:54:44.775189802 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 3] ProcessGroupNCCL initialization options: size: 7, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank2]:[I822 08:54:44.775194169 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 2] ProcessGroupNCCL initialization options: size: 7, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank3]:[I822 08:54:44.775198710 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 08:54:44.775201395 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 08:54:44.775202556 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 1] ProcessGroupNCCL initialization options: size: 7, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank1]:[I822 08:54:44.775211508 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 08:54:44.775253326 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 7, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank0]:[I822 08:54:44.775263807 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=5 pid=117)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=6 pid=118)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m WorkerProc failed to start.
[1;36m(VllmWorker rank=2 pid=114)[0;0m WorkerProc failed to start.
WorkerProc failed to start.
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m Traceback (most recent call last):
[1;36m(VllmWorker rank=5 pid=117)[0;0m WorkerProc failed to start.
Traceback (most recent call last):
[1;36m(VllmWorker rank=3 pid=115)[0;0m Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
[1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
[1;36m(VllmWorker rank=3 pid=115)[0;0m Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
[1;36m(VllmWorker rank=0 pid=112)[0;0m     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
[1;36m(VllmWorker rank=0 pid=112)[0;0m     worker = WorkerProc(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=112)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    self.worker.load_model()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
    self.worker.load_model()
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
WorkerProc failed to start.
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
    self.worker.load_model()
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m     self.worker.load_model()
    self.model_runner.load_model()
    self.model_runner.load_model()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m Traceback (most recent call last):
[1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
[1;36m(VllmWorker rank=6 pid=118)[0;0m     self.model_runner.load_model()
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
    self.model_runner.load_model()
    self.model = model_loader.load_model(
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
    self.model = model_loader.load_model(
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
    worker = WorkerProc(*args, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^
    self.model = model_loader.load_model(
[1;36m(VllmWorker rank=2 pid=114)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     self.model = model_loader.load_model(
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
                 ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=5 pid=117)[0;0m     model = initialize_model(vllm_config=vllm_config,
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
    model = initialize_model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    self.worker.load_model()
[1;36m(VllmWorker rank=2 pid=114)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    model = initialize_model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m     model = initialize_model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
[1;36m(VllmWorker rank=0 pid=112)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
    self.model_runner.load_model()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
    return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     self.model = model_loader.load_model(
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
[1;36m(VllmWorker rank=6 pid=118)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m     self.model = Qwen2Model(vllm_config=vllm_config,
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
[1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=0 pid=112)[0;0m     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
[1;36m(VllmWorker rank=2 pid=114)[0;0m     model = initialize_model(vllm_config=vllm_config,
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
[1;36m(VllmWorker rank=0 pid=112)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m     self.embed_tokens = VocabParallelEmbedding(
    self.embed_tokens = VocabParallelEmbedding(
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
                        ^^^^^^^^^^^^^^^^^^^^^^^
    self.embed_tokens = VocabParallelEmbedding(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     self.embed_tokens = VocabParallelEmbedding(
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
[1;36m(VllmWorker rank=2 pid=114)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
[1;36m(VllmWorker rank=5 pid=117)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
    self.shard_indices = self._get_indices(self.num_embeddings_padded,
[1;36m(VllmWorker rank=6 pid=118)[0;0m     self.shard_indices = self._get_indices(self.num_embeddings_padded,
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
    self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=3 pid=115)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    self.shard_indices = self._get_indices(self.num_embeddings_padded,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
    self.shard_indices = self._get_indices(self.num_embeddings_padded,
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
    vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
[1;36m(VllmWorker rank=5 pid=117)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
    vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
    vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
    vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m     per_partition_vocab_size = divide(global_vocab_size, world_size)
    self.embed_tokens = VocabParallelEmbedding(
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     per_partition_vocab_size = divide(global_vocab_size, world_size)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                        ^^^^^^^^^^^^^^^^^^^^^^^
    per_partition_vocab_size = divide(global_vocab_size, world_size)
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m     per_partition_vocab_size = divide(global_vocab_size, world_size)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=0 pid=112)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m     self.shard_indices = self._get_indices(self.num_embeddings_padded,
    ensure_divisibility(numerator, denominator)
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     ensure_divisibility(numerator, denominator)
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m     ensure_divisibility(numerator, denominator)
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
[1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
[1;36m(VllmWorker rank=0 pid=112)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
[1;36m(VllmWorker rank=6 pid=118)[0;0m     ensure_divisibility(numerator, denominator)
[1;36m(VllmWorker rank=2 pid=114)[0;0m     assert numerator % denominator == 0, "{} is not divisible by {}".format(
[1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     assert numerator % denominator == 0, "{} is not divisible by {}".format(
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
[1;36m(VllmWorker rank=6 pid=118)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
    vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
[1;36m(VllmWorker rank=0 pid=112)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m AssertionError: 152064 is not divisible by 7
[1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=2 pid=114)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
AssertionError: 152064 is not divisible by 7
    assert numerator % denominator == 0, "{} is not divisible by {}".format(
AssertionError: 152064 is not divisible by 7
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m     per_partition_vocab_size = divide(global_vocab_size, world_size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=3 pid=115)[0;0m AssertionError: 152064 is not divisible by 7
[1;36m(VllmWorker rank=6 pid=118)[0;0m                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
[1;36m(VllmWorker rank=6 pid=118)[0;0m     ensure_divisibility(numerator, denominator)
[1;36m(VllmWorker rank=6 pid=118)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
[1;36m(VllmWorker rank=6 pid=118)[0;0m     assert numerator % denominator == 0, "{} is not divisible by {}".format(
[1;36m(VllmWorker rank=6 pid=118)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=6 pid=118)[0;0m AssertionError: 152064 is not divisible by 7
[1;36m(VllmWorker rank=4 pid=116)[0;0m WorkerProc failed to start.
[1;36m(VllmWorker rank=4 pid=116)[0;0m Traceback (most recent call last):
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 461, in worker_main
[1;36m(VllmWorker rank=4 pid=116)[0;0m     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker rank=4 pid=116)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 358, in __init__
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.worker.load_model()
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 185, in load_model
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.model_runner.load_model()
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1776, in load_model
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.model = model_loader.load_model(
[1;36m(VllmWorker rank=4 pid=116)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 38, in load_model
[1;36m(VllmWorker rank=4 pid=116)[0;0m     model = initialize_model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=4 pid=116)[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py", line 64, in initialize_model
[1;36m(VllmWorker rank=4 pid=116)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorker rank=4 pid=116)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 448, in __init__
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.model = Qwen2Model(vllm_config=vllm_config,
[1;36m(VllmWorker rank=4 pid=116)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 152, in __init__
[1;36m(VllmWorker rank=4 pid=116)[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 306, in __init__
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.embed_tokens = VocabParallelEmbedding(
[1;36m(VllmWorker rank=4 pid=116)[0;0m                         ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 225, in __init__
[1;36m(VllmWorker rank=4 pid=116)[0;0m     self.shard_indices = self._get_indices(self.num_embeddings_padded,
[1;36m(VllmWorker rank=4 pid=116)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 283, in _get_indices
[1;36m(VllmWorker rank=4 pid=116)[0;0m     vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank,
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py", line 72, in vocab_range_from_global_vocab_size
[1;36m(VllmWorker rank=4 pid=116)[0;0m     per_partition_vocab_size = divide(global_vocab_size, world_size)
[1;36m(VllmWorker rank=4 pid=116)[0;0m                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 57, in divide
[1;36m(VllmWorker rank=4 pid=116)[0;0m     ensure_divisibility(numerator, denominator)
[1;36m(VllmWorker rank=4 pid=116)[0;0m   File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/distributed/utils.py", line 50, in ensure_divisibility
[1;36m(VllmWorker rank=4 pid=116)[0;0m     assert numerator % denominator == 0, "{} is not divisible by {}".format(
[1;36m(VllmWorker rank=4 pid=116)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=4 pid=116)[0;0m AssertionError: 152064 is not divisible by 7
[rank1]:[I822 08:54:46.920401472 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 08:54:46.920433911 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 1] Launching ProcessGroupNCCL abort asynchrounously.
[rank1]:[I822 08:54:46.921535418 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL aborts successfully.
[rank1]:[I822 08:54:46.921593453 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL watchdog thread joined.
[rank1]:[I822 08:54:46.921609542 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 08:54:46.929776357 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 08:54:46.929807305 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 2] Launching ProcessGroupNCCL abort asynchrounously.
[rank2]:[I822 08:54:46.929957184 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL aborts successfully.
[rank2]:[I822 08:54:46.930006183 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL watchdog thread joined.
[rank2]:[I822 08:54:46.930023261 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 08:54:46.930698865 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 08:54:46.930725301 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 5] Launching ProcessGroupNCCL abort asynchrounously.
[rank5]:[I822 08:54:46.930847803 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL aborts successfully.
[rank5]:[I822 08:54:46.930898790 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL watchdog thread joined.
[rank5]:[I822 08:54:46.930915241 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 08:54:46.934227809 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 08:54:46.934255069 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 6] Launching ProcessGroupNCCL abort asynchrounously.
[rank6]:[I822 08:54:46.934416536 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL aborts successfully.
[rank6]:[I822 08:54:46.934464593 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL watchdog thread joined.
[rank6]:[I822 08:54:46.934482463 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 08:54:46.935402521 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 08:54:46.935428426 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 3] Launching ProcessGroupNCCL abort asynchrounously.
[rank3]:[I822 08:54:46.935545930 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL aborts successfully.
[rank3]:[I822 08:54:46.935589412 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL watchdog thread joined.
[rank3]:[I822 08:54:46.935604419 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 08:54:46.942820181 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 08:54:46.942849163 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 4] Launching ProcessGroupNCCL abort asynchrounously.
[rank4]:[I822 08:54:46.942983118 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL aborts successfully.
[rank4]:[I822 08:54:46.943026870 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL watchdog thread joined.
[rank4]:[I822 08:54:46.943043891 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 08:54:46.034593919 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[W822 08:54:46.034638134 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[I822 08:54:46.034654828 ProcessGroupNCCL.cpp:1389] [PG ID 0 PG GUID 0 Rank 0] Launching ProcessGroupNCCL abort asynchrounously.
[rank0]:[I822 08:54:46.034822762 ProcessGroupNCCL.cpp:1397] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL aborts successfully.
[rank0]:[I822 08:54:46.034887347 ProcessGroupNCCL.cpp:1513] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL watchdog thread joined.
[rank0]:[I822 08:54:46.034910710 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 08:54:47.917726344 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank0]:[I822 08:54:47.918308737 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank0]:[I822 08:54:47.918766593 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank0]:[I822 08:54:47.918874469 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank0]:[I822 08:54:47.918914870 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank0]:[I822 08:54:47.919018325 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
EngineCore failed to start.
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 75, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 93, in _init_executor
    self.workers = WorkerProc.wait_for_ready(unready_workers)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 422, in wait_for_ready
    raise e from None
Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Process EngineCore_0:
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 590, in run_engine_core
    raise e
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 577, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 404, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 75, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 93, in _init_executor
    self.workers = WorkerProc.wait_for_ready(unready_workers)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 422, in wait_for_ready
    raise e from None
Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 149, in <module>
    main()
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 135, in main
    pipeline = Pipeline(
               ^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/pipeline.py", line 156, in __init__
    self.model = self._init_model(model_config, model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/pipeline.py", line 200, in _init_model
    return load_model(config=model_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/models/model_loader.py", line 107, in load_model
    return load_model_with_accelerate_or_default(config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/models/model_loader.py", line 198, in load_model_with_accelerate_or_default
    model = VLLMModel(config=config)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/models/vllm/vllm_model.py", line 114, in __init__
    self.model = self._create_auto_model(config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/lighteval/models/vllm/vllm_model.py", line 184, in _create_auto_model
    model = LLM(**self.model_args)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 271, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 501, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 124, in from_vllm_config
    return cls(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 101, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 503, in __init__
    super().__init__(
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 403, in __init__
    with launch_core_engines(vllm_config, executor_class,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 434, in launch_core_engines
    wait_for_engine_startup(
  File "/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 484, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
---- 2025-08-22T08:55:04+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 7 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 08:55:06.607000188 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:55:53 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 08:56:52.712443344 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 08:57:36 [__init__.py:244] Automatically detected platform cuda.
[I822 08:57:48.614873278 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.614873904 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.614885140 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.614867440 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.614870875 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.614871170 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 08:57:48.615571445 debug.cpp:50] [c10d] The debug level is set to INFO.
---- 2025-08-22T08:59:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 08:59:19.920164010 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:00:30 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 09:01:34.937177417 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:02:19 [__init__.py:244] Automatically detected platform cuda.
[I822 09:02:30.992195662 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:02:30.992195534 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:02:30.992195556 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:02:30.992897994 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:03:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:03:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:03:19 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:03:19 [__init__.py:244] Automatically detected platform cuda.
[I822 09:03:35.682816223 TCPStore.cpp:274] [c10d - debug] The server has started on port = 33135.
[I822 09:03:35.682835764 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 09:03:35.682939656 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 33135).
[I822 09:03:35.685233966 socket.cpp:946] [c10d] The client socket has connected to [localhost]:33135 on SocketImpl(fd=124, addr=[localhost]:54492, remote=[localhost]:33135).
[I822 09:03:35.694549408 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:33135
[I822 09:03:36.779467896 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 33135).
[I822 09:03:36.782975193 socket.cpp:946] [c10d] The client socket has connected to [localhost]:33135 on SocketImpl(fd=116, addr=[localhost]:54504, remote=[localhost]:33135).
[I822 09:03:36.786433752 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:33135
[I822 09:03:36.792912115 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:03:36.792926932 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:03:36.797855102 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 09:03:36.797873378 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:03:36.895360853 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 33135).
[I822 09:03:36.903833046 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 33135).
[I822 09:03:36.897401237 socket.cpp:946] [c10d] The client socket has connected to [localhost]:33135 on SocketImpl(fd=116, addr=[localhost]:54510, remote=[localhost]:33135).
[I822 09:03:36.904035550 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:33135
[I822 09:03:36.907135322 socket.cpp:946] [c10d] The client socket has connected to [localhost]:33135 on SocketImpl(fd=116, addr=[localhost]:54520, remote=[localhost]:33135).
[I822 09:03:36.910828664 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:33135
[I822 09:03:36.923331382 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:03:36.923343803 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:03:36.923361625 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:03:36.923372899 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:03:36.923686515 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 09:03:36.923694409 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:03:36.923747198 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 09:03:36.923755708 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:03:36.933397665 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:03:36.933410936 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:03:36.933708736 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 09:03:36.933716431 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:03:36.008080097 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 09:03:36.008082684 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 09:03:36.008084710 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 09:03:36.008090737 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:03:36.008091864 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:03:36.008091398 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:03:36.008092783 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 09:03:36.008099740 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:112:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853416::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755853416::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 09:05:07.763921083 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 09:05:07.763951806 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:05:07.763945043 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank1]:[I822 09:05:07.763972228 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:05:07.763974558 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank3]:[I822 09:05:07.763980992 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank2]:[I822 09:05:07.764015335 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:05:07.764022101 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:05:07.765042447 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank3]:[I822 09:05:07.765050647 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:05:07.765050991 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank0]:[I822 09:05:07.765057999 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:05:07.765074530 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank2]:[I822 09:05:07.765081715 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:05:07.765082233 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank1]:[I822 09:05:07.765090283 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:05:07.765792390 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank3]:[I822 09:05:07.765798868 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:05:07.765837247 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank2]:[I822 09:05:07.765844778 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:05:07.765900060 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank0]:[I822 09:05:07.765903687 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank1]:[I822 09:05:07.765907502 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:05:07.765910932 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=3 pid=115)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:44<00:44, 44.33s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:16<00:00, 37.25s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:16<00:00, 38.31s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank0]:[I822 09:07:39.451853716 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank2]:[I822 09:07:39.451855170 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank3]:[I822 09:07:39.451850504 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank1]:[I822 09:07:39.451850680 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank0]:[I822 09:07:39.452129502 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.027956 ms
[rank0]:[I822 09:07:39.452172025 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:07:39.452224282 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.340955 ms
[rank2]:[I822 09:07:39.452249087 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank3]:[I822 09:07:39.452239142 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.347265 ms
[rank3]:[I822 09:07:39.452259864 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank1]:[I822 09:07:39.452252689 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.359004 ms
[rank1]:[I822 09:07:39.452274521 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:07:39.703266130 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank0]:[I822 09:07:39.703347726 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank1]:[I822 09:07:39.703382212 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank3]:[I822 09:07:39.703397022 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:31,  2.10it/s]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:00<00:29,  2.19it/s]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:01<00:28,  2.23it/s]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:01<00:28,  2.24it/s]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:02<00:27,  2.25it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:02<00:27,  2.25it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:03<00:26,  2.25it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:03<00:26,  2.26it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:04<00:25,  2.26it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:04<00:25,  2.26it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:04<00:24,  2.26it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:05<00:24,  2.26it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:05<00:23,  2.26it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:06<00:23,  2.26it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:06<00:22,  2.26it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:07<00:22,  2.26it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:07<00:22,  2.27it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:07<00:21,  2.28it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:08<00:21,  2.28it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:08<00:20,  2.29it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:09<00:20,  2.28it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:09<00:19,  2.27it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:10<00:19,  2.26it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:10<00:19,  2.26it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:11<00:18,  2.25it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:11<00:18,  2.25it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:11<00:17,  2.25it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:12<00:17,  2.25it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:12<00:16,  2.25it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:13<00:16,  2.25it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:13<00:15,  2.25it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:14<00:15,  2.25it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:14<00:15,  2.25it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:15<00:14,  2.25it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:15<00:14,  2.25it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:15<00:13,  2.25it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:16<00:13,  2.25it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:16<00:12,  2.25it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:17<00:12,  2.25it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:17<00:11,  2.26it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:18<00:11,  2.26it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:18<00:11,  2.25it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:19<00:10,  2.25it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:19<00:10,  2.24it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:19<00:09,  2.24it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:20<00:09,  2.23it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:20<00:08,  2.23it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:21<00:08,  2.23it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:21<00:08,  2.20it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:22<00:07,  2.21it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:22<00:07,  2.21it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:23<00:06,  2.21it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:23<00:06,  2.21it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:24<00:05,  2.22it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:24<00:05,  2.22it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:24<00:04,  2.21it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:25<00:04,  2.19it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:25<00:04,  2.20it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:26<00:03,  2.20it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:26<00:03,  2.21it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:27<00:02,  2.18it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:27<00:02,  2.20it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:28<00:01,  2.21it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:28<00:01,  2.21it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:29<00:00,  2.21it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:29<00:00,  2.22it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:29<00:00,  2.18it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:29<00:00,  2.24it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 30/30 [00:00<00:00, 2984.28it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|▎         | 1/30 [00:16<08:06, 16.77s/it, est. speed input: 12.35 toks/s, output: 121.79 toks/s][A
Processed prompts:   7%|▋         | 2/30 [00:21<04:25,  9.49s/it, est. speed input: 16.59 toks/s, output: 217.60 toks/s][A
Processed prompts:  10%|█         | 3/30 [00:21<02:24,  5.34s/it, est. speed input: 26.30 toks/s, output: 334.65 toks/s][A
Processed prompts:  13%|█▎        | 4/30 [00:22<01:35,  3.68s/it, est. speed input: 31.03 toks/s, output: 438.98 toks/s][A
Processed prompts:  17%|█▋        | 5/30 [00:24<01:10,  2.84s/it, est. speed input: 34.70 toks/s, output: 535.16 toks/s][A
Processed prompts:  20%|██        | 6/30 [00:25<00:54,  2.27s/it, est. speed input: 42.05 toks/s, output: 630.98 toks/s][A
Processed prompts:  23%|██▎       | 7/30 [00:26<00:47,  2.06s/it, est. speed input: 46.75 toks/s, output: 713.41 toks/s][A
Processed prompts:  27%|██▋       | 8/30 [00:31<01:04,  2.94s/it, est. speed input: 45.37 toks/s, output: 726.38 toks/s][A
Processed prompts:  30%|███       | 9/30 [00:35<01:07,  3.24s/it, est. speed input: 46.64 toks/s, output: 767.92 toks/s][A
Processed prompts:  33%|███▎      | 10/30 [00:39<01:09,  3.47s/it, est. speed input: 47.86 toks/s, output: 811.69 toks/s][A
Processed prompts:  37%|███▋      | 11/30 [00:41<00:55,  2.92s/it, est. speed input: 49.36 toks/s, output: 899.33 toks/s][A
Processed prompts:  40%|████      | 12/30 [00:56<01:57,  6.55s/it, est. speed input: 39.66 toks/s, output: 781.44 toks/s][A
Processed prompts:  43%|████▎     | 13/30 [00:59<01:36,  5.70s/it, est. speed input: 41.86 toks/s, output: 852.66 toks/s][A
Processed prompts:  47%|████▋     | 14/30 [01:00<01:05,  4.08s/it, est. speed input: 44.40 toks/s, output: 968.04 toks/s][A
Processed prompts:  50%|█████     | 15/30 [01:19<02:10,  8.71s/it, est. speed input: 35.47 toks/s, output: 851.28 toks/s][A
Processed prompts:  53%|█████▎    | 16/30 [01:29<02:06,  9.05s/it, est. speed input: 33.09 toks/s, output: 877.18 toks/s][A
Processed prompts:  57%|█████▋    | 17/30 [05:13<15:59, 73.82s/it, est. speed input: 10.94 toks/s, output: 354.34 toks/s][A
Processed prompts: 100%|██████████| 30/30 [05:13<00:00, 73.82s/it, est. speed input: 18.16 toks/s, output: 1711.33 toks/s][AProcessed prompts: 100%|██████████| 30/30 [05:13<00:00, 10.46s/it, est. speed input: 18.16 toks/s, output: 1711.33 toks/s]
Splits: 100%|██████████| 1/1 [05:14<00:00, 314.18s/it]Splits: 100%|██████████| 1/1 [05:14<00:00, 314.18s/it]
[rank0]:[I822 09:13:29.061993030 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.062012716 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.062164456 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:13:29.062800388 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.064388665 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.064409405 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.064487608 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.064521014 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.064598468 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:13:29.064710361 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:13:29.065120585 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:13:29.065193611 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.469205593 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank3]:[I822 09:13:29.474575031 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank0]:[I822 09:13:29.477029192 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank1]:[I822 09:13:29.480085471 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank0]:[I822 09:13:29.503941509 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:13:29.503963650 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.504936965 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:13:29.504944666 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.504998758 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:13:29.505004288 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 09:13:29.505024906 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:13:29.505051541 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:13:29.514639115 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.514659864 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:13:29.514699791 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.514705447 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:13:29.514753967 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.514759427 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 09:13:29.514778144 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.514793498 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.523021710 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:13:29.523029761 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:13:29.523048454 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:13:29.523077974 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.523080186 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:13:29.523083312 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 13 Rank 0] Destroy complete.
[rank0]:[I822 09:13:29.523095770 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:13:29.523111876 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:13:29.523126987 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.523131052 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:13:29.523196633 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.523204423 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 09:13:29.523228577 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:13:29.523250344 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.524042811 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.524069651 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.524107443 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:13:29.524112526 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:13:29.524160319 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:13:29.524165632 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 09:13:29.524185692 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.524203982 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:13:29.533063826 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.533071610 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:13:29.533109739 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.533114427 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 17 Rank 0] Destroy complete.
[rank2]:[I822 09:13:29.533126699 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.533140161 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:13:29.541184126 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.541194105 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:13:29.541247425 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.541252188 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 15 Rank 0] Destroy complete.
[rank1]:[I822 09:13:29.541266797 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:13:29.541286478 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.543015622 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:13:29.543026282 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:13:29.543069779 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:13:29.543073879 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 19 Rank 0] Destroy complete.
[rank3]:[I822 09:13:29.543085474 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.543099263 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.543145853 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:13:29.543153171 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.543203207 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:13:29.543207376 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 0] Destroy complete.
[rank2]:[I822 09:13:29.545212856 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.545221097 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:13:29.545269222 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.545273571 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 2] Destroy complete.
[rank3]:[I822 09:13:29.555515589 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:13:29.555524343 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:13:29.555581628 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:13:29.555586360 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 3] Destroy complete.
[rank1]:[I822 09:13:29.560596734 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.560607977 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:13:29.560665088 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.560669433 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 1] Destroy complete.
[rank0]:[I822 09:13:29.578121887 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:13:29.578138787 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.578155818 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:13:29.578158436 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.578198316 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:13:29.578203450 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 09:13:29.578217098 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:13:29.578230030 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:13:29.588165799 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:13:29.588184062 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:13:29.588201928 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.588204854 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:13:29.588259479 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.588265243 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank1]:[I822 09:13:29.588280736 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:13:29.588301951 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:13:29.595468328 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.595488425 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:13:29.595505556 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.595508472 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:13:29.595577930 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.595584887 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank2]:[I822 09:13:29.595598935 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.595612263 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.598143615 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.598162538 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.598182348 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:13:29.598186844 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:13:29.598230711 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:13:29.598236139 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank3]:[I822 09:13:29.598251256 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.598265113 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:13:29.623984357 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:13:29.623993817 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:13:29.624069409 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:13:29.624075601 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[rank1]:[I822 09:13:29.624084123 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:13:29.624090500 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[I822 09:13:29.624118949 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:13:29.624141589 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:13:29.624149265 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[I822 09:13:29.624153947 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:13:29.624163863 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:13:29.624173909 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[rank2]:[I822 09:13:29.624201021 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:13:29.624210985 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[I822 09:13:29.624220008 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:13:29.624223533 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[I822 09:13:29.624196829 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[rank3]:[I822 09:13:29.624230421 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[I822 09:13:29.624241187 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:13:29.624249360 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 09:13:29.624269355 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:13:29.624275026 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[I822 09:13:29.624274638 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[rank3]:[I822 09:13:29.624277558 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 09:13:29.624284511 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 09:13:29.624304591 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 09:13:29.624310611 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 09:13:29.624313637 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[I822 09:13:29.624314279 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:13:29.624316951 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:13:29.624318814 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:13:29.624330477 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:13:29.624345864 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 09:13:29.624348370 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:13:29.624350339 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:13:29.624352242 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:13:29.624354574 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 09:13:29.624384414 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
|     Task      |Version|     Metric     |Value|   |Stderr|
|---------------|------:|----------------|----:|---|-----:|
|all            |       |extractive_match|  0.5|±  |0.0928|
|custom:aime24:0|      1|extractive_match|  0.5|±  |0.0928|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 25.67ba/s]
+ set +x
---- 2025-08-22T09:13:38+00:00 RUN END ----
---- 2025-08-22T23:27:35+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:27:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:28:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:29:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:29:26.260690565 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:30:13 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 23:31:11.584116605 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:31:56 [__init__.py:244] Automatically detected platform cuda.
[I822 23:32:08.849179438 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:32:08.849197322 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:32:08.849218725 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:32:08.849953456 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:32:57 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:57 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:57 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:32:57 [__init__.py:244] Automatically detected platform cuda.
[I822 23:33:12.409528978 TCPStore.cpp:274] [c10d - debug] The server has started on port = 43635.
[I822 23:33:12.409550226 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 23:33:12.409643827 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 43635).
[I822 23:33:12.412806496 socket.cpp:946] [c10d] The client socket has connected to [localhost]:43635 on SocketImpl(fd=124, addr=[localhost]:58606, remote=[localhost]:43635).
[I822 23:33:12.419266094 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:43635
[I822 23:33:12.516025269 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 43635).
[I822 23:33:12.516664757 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 43635).
[I822 23:33:12.517485283 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 43635).
[I822 23:33:12.520744348 socket.cpp:946] [c10d] The client socket has connected to [localhost]:43635 on SocketImpl(fd=116, addr=[localhost]:58630, remote=[localhost]:43635).
[I822 23:33:12.523209063 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:43635
[I822 23:33:12.519986532 socket.cpp:946] [c10d] The client socket has connected to [localhost]:43635 on SocketImpl(fd=116, addr=[localhost]:58624, remote=[localhost]:43635).
[I822 23:33:12.524513895 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:43635
[I822 23:33:12.519123532 socket.cpp:946] [c10d] The client socket has connected to [localhost]:43635 on SocketImpl(fd=116, addr=[localhost]:58616, remote=[localhost]:43635).
[I822 23:33:12.525440339 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:43635
[I822 23:33:12.564853908 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:12.564855858 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:12.564865781 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:12.564867873 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:12.564873475 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:12.564887169 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:12.565170256 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 23:33:12.565179318 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:33:12.565188292 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 23:33:12.565190662 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 23:33:12.565193889 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:12.565196254 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:12.574949104 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:12.574963283 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:12.575263350 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 23:33:12.575270898 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:12.580598833 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 23:33:12.580608212 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:12.580609881 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 23:33:12.580616419 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:12.580634307 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 23:33:12.580640698 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:33:12.580645521 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 23:33:12.580652393 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:114:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905593::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905593::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 23:34:44.182001991 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 23:34:44.182030794 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:44.182053225 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank2]:[I822 23:34:44.182058795 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank1]:[I822 23:34:44.182075642 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:44.182082081 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:44.182082909 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank3]:[I822 23:34:44.182105578 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:34:44.183100489 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank3]:[I822 23:34:44.183104640 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank0]:[I822 23:34:44.183110018 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:44.183108800 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank3]:[I822 23:34:44.183112148 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:44.183115812 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:44.183148183 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank2]:[I822 23:34:44.183158306 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:44.183849678 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL initialization options: size: 4, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank3]:[I822 23:34:44.183858654 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:44.183876696 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL initialization options: size: 4, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank2]:[I822 23:34:44.183885927 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:44.183899281 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL initialization options: size: 4, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank1]:[I822 23:34:44.183906015 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:34:44.183924150 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 4, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank0]:[I822 23:34:44.183933239 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=0 pid=112)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m [1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.56s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.30s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.34s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank1]:[I822 23:36:00.894341205 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank0]:[I822 23:36:00.894843794 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank0]:[I822 23:36:00.895157762 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.045555 ms
[rank0]:[I822 23:36:00.895188637 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank1]:[I822 23:36:00.895327707 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.93835 ms
[rank1]:[I822 23:36:00.895373747 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank2]:[I822 23:36:00.895721919 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank2]:[I822 23:36:00.895869859 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.102361 ms
[rank2]:[I822 23:36:00.895900206 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank3]:[I822 23:36:00.895948587 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank3]:[I822 23:36:00.896167933 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.176648 ms
[rank3]:[I822 23:36:00.896198473 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank0]:[I822 23:36:00.328108088 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank1]:[I822 23:36:00.328154621 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank3]:[I822 23:36:00.328503440 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank2]:[I822 23:36:00.329745047 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:33,  1.98it/s]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:00<00:30,  2.14it/s]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:01<00:29,  2.21it/s]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:01<00:28,  2.24it/s]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:02<00:27,  2.24it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:02<00:27,  2.24it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:03<00:26,  2.24it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:03<00:26,  2.26it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:04<00:25,  2.32it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:04<00:24,  2.36it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:04<00:23,  2.40it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:05<00:22,  2.43it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:05<00:22,  2.44it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:06<00:21,  2.44it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:06<00:21,  2.45it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:06<00:20,  2.46it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:07<00:20,  2.47it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:07<00:19,  2.47it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:08<00:19,  2.42it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:08<00:20,  2.35it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:08<00:20,  2.30it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:09<00:19,  2.26it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:09<00:19,  2.24it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:10<00:19,  2.23it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:10<00:18,  2.22it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:11<00:18,  2.21it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:11<00:18,  2.20it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:12<00:17,  2.20it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:12<00:17,  2.19it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:13<00:16,  2.19it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:13<00:16,  2.20it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:13<00:15,  2.24it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:14<00:14,  2.30it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:14<00:14,  2.34it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:15<00:13,  2.38it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:15<00:12,  2.40it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:16<00:12,  2.40it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:16<00:11,  2.43it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:16<00:11,  2.44it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:17<00:11,  2.45it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:17<00:10,  2.46it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:18<00:10,  2.46it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:18<00:10,  2.38it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:18<00:09,  2.31it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:19<00:09,  2.27it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:19<00:09,  2.24it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:20<00:09,  2.22it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:20<00:08,  2.21it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:21<00:08,  2.18it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:21<00:07,  2.18it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:22<00:07,  2.17it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:22<00:06,  2.17it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:23<00:06,  2.17it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:23<00:05,  2.17it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:24<00:05,  2.18it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:24<00:04,  2.23it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:24<00:04,  2.29it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:25<00:03,  2.33it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:25<00:03,  2.36it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:26<00:02,  2.38it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:26<00:02,  2.36it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:26<00:02,  2.39it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:27<00:01,  2.40it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:27<00:01,  2.41it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:28<00:00,  2.38it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:28<00:00,  2.31it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:29<00:00,  2.25it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:29<00:00,  2.30it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 30/30 [00:00<00:00, 8523.85it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|▎         | 1/30 [00:14<07:13, 14.94s/it, est. speed input: 14.45 toks/s, output: 121.39 toks/s][A
Processed prompts:   7%|▋         | 2/30 [00:17<03:28,  7.44s/it, est. speed input: 21.00 toks/s, output: 227.14 toks/s][A
Processed prompts:  10%|█         | 3/30 [00:18<02:01,  4.50s/it, est. speed input: 31.25 toks/s, output: 335.84 toks/s][A
Processed prompts:  13%|█▎        | 4/30 [00:21<01:50,  4.24s/it, est. speed input: 32.04 toks/s, output: 398.44 toks/s][A
Processed prompts:  17%|█▋        | 5/30 [00:23<01:24,  3.40s/it, est. speed input: 34.93 toks/s, output: 487.56 toks/s][A
Processed prompts:  20%|██        | 6/30 [00:24<00:57,  2.38s/it, est. speed input: 41.80 toks/s, output: 600.34 toks/s][A
Processed prompts:  23%|██▎       | 7/30 [00:24<00:39,  1.70s/it, est. speed input: 50.46 toks/s, output: 713.70 toks/s][A
Processed prompts:  27%|██▋       | 8/30 [00:31<01:11,  3.23s/it, est. speed input: 47.05 toks/s, output: 685.97 toks/s][A
Processed prompts:  30%|███       | 9/30 [00:39<01:45,  5.00s/it, est. speed input: 40.15 toks/s, output: 654.49 toks/s][A
Processed prompts:  33%|███▎      | 10/30 [00:49<02:05,  6.29s/it, est. speed input: 36.06 toks/s, output: 653.16 toks/s][A
Processed prompts:  37%|███▋      | 11/30 [01:07<03:09,  9.96s/it, est. speed input: 28.66 toks/s, output: 595.36 toks/s][A
Processed prompts:  40%|████      | 12/30 [01:10<02:23,  7.99s/it, est. speed input: 29.63 toks/s, output: 684.82 toks/s][A
Processed prompts:  43%|████▎     | 13/30 [01:13<01:46,  6.25s/it, est. speed input: 30.70 toks/s, output: 782.52 toks/s][A
Processed prompts:  47%|████▋     | 14/30 [01:23<01:59,  7.45s/it, est. speed input: 30.30 toks/s, output: 804.76 toks/s][A
Processed prompts:  50%|█████     | 15/30 [01:24<01:24,  5.63s/it, est. speed input: 31.67 toks/s, output: 909.52 toks/s][A
Processed prompts:  53%|█████▎    | 16/30 [01:38<01:50,  7.91s/it, est. speed input: 29.33 toks/s, output: 904.55 toks/s][A
Processed prompts:  57%|█████▋    | 17/30 [01:56<02:23, 11.01s/it, est. speed input: 25.90 toks/s, output: 879.63 toks/s][A
Processed prompts:  60%|██████    | 18/30 [04:50<12:01, 60.11s/it, est. speed input: 11.99 toks/s, output: 458.22 toks/s][A
Processed prompts:  63%|██████▎   | 19/30 [05:10<08:48, 48.08s/it, est. speed input: 12.06 toks/s, output: 534.07 toks/s][A
Processed prompts: 100%|██████████| 30/30 [05:10<00:00, 48.08s/it, est. speed input: 18.35 toks/s, output: 1693.98 toks/s][AProcessed prompts: 100%|██████████| 30/30 [05:10<00:00, 10.36s/it, est. speed input: 18.35 toks/s, output: 1693.98 toks/s]
Splits: 100%|██████████| 1/1 [05:11<00:00, 311.03s/it]Splits: 100%|██████████| 1/1 [05:11<00:00, 311.03s/it]
[rank0]:[I822 23:41:50.600082271 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:50.600192306 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:50.600230207 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:50.601081199 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:50.602479793 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:41:50.602560988 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:50.602594891 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:50.602607568 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:50.602715331 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:50.602722519 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:41:50.603473325 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:50.603590005 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.043746937 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank0]:[I822 23:41:51.047233396 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank1]:[I822 23:41:51.053023791 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank2]:[I822 23:41:51.053039971 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank1]:[I822 23:41:51.083710957 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:41:51.083735552 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.083732714 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.083752621 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:41:51.083782837 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:51.083786988 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:41:51.083801444 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:51.083805489 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.083850698 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.083851253 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.083855742 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank1]:[I822 23:41:51.083859059 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank0]:[I822 23:41:51.083874477 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:41:51.083882435 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.083888863 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:41:51.083900491 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:41:51.093701987 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.093724746 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:41:51.093765111 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:51.093770390 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:41:51.093817024 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.093831037 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 23:41:51.093851343 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.093866076 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.096906086 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:51.096914321 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:41:51.096971038 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.096975319 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 13 Rank 0] Destroy complete.
[rank0]:[I822 23:41:51.096987697 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.097001518 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:41:51.097234641 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:51.097244735 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.097304597 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:41:51.097309491 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 15 Rank 0] Destroy complete.
[rank1]:[I822 23:41:51.097324451 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:41:51.097342654 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.098385986 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:41:51.098410211 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.098449480 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:51.098454553 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:51.098505217 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:41:51.098510337 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 23:41:51.098530923 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:41:51.098547035 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.110544462 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:51.110552306 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:41:51.110593286 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.110597563 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 0] Destroy complete.
[rank3]:[I822 23:41:51.111269698 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:51.111278089 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:41:51.111319426 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.111323508 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 19 Rank 0] Destroy complete.
[rank3]:[I822 23:41:51.111336974 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.111349879 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:41:51.117180355 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:51.117189964 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.117243921 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:41:51.117246837 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 1] Destroy complete.
[rank2]:[I822 23:41:51.118345288 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:51.118355917 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:51.118396550 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:41:51.118400786 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 17 Rank 0] Destroy complete.
[rank2]:[I822 23:41:51.118413352 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:41:51.118426526 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:41:51.130327275 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:51.130335267 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:41:51.130392074 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.130403890 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 3] Destroy complete.
[rank2]:[I822 23:41:51.136769266 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 21 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:51.136777822 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 21 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:51.136817824 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 21 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:41:51.136821857 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 21 Rank 2] Destroy complete.
[rank0]:[I822 23:41:51.153880015 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.153897328 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.153916525 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:51.153921312 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.153970631 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.153975181 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.153979180 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank1]:[I822 23:41:51.153994093 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.153994878 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:41:51.154006090 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:41:51.154013213 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:51.154016137 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.154072606 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:41:51.154078536 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank1]:[I822 23:41:51.154093855 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:41:51.154114415 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.164055955 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:41:51.164075143 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.164091579 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:51.164094177 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:41:51.164116191 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.164135763 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 21 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.164140800 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:41:51.164144225 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank2]:[I822 23:41:51.164159583 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.164160653 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:51.164163699 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:51.164174580 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:41:51.164217493 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.164222922 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank3]:[I822 23:41:51.164239038 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:41:51.164250106 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:41:51.185035166 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:41:51.185046067 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:41:51.185087818 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:41:51.185092172 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[I822 23:41:51.185126016 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 23:41:51.185143609 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:41:51.185190099 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 23:41:51.185262071 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 23:41:51.185268620 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 23:41:51.185271625 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 23:41:51.185275018 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:41:51.185277524 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:41:51.185279722 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:41:51.185281630 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:41:51.185310350 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 23:41:51.185314344 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:41:51.185316452 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:41:51.185318349 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:41:51.185320272 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:41:51.185322671 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 23:41:51.185349402 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank1]:[I822 23:41:51.203924192 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:41:51.203935803 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:41:51.203996857 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:41:51.204016850 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 23:41:51.204061501 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 23:41:51.204082240 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:41:51.205110845 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:41:51.205123958 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:41:51.205168881 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:41:51.205174029 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 23:41:51.205207808 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[I822 23:41:51.205225308 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:41:51.211176209 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:41:51.211186856 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:41:51.211230602 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:41:51.211237689 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[I822 23:41:51.211284572 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 23:41:51.211300242 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value |   |Stderr|
|---------------|------:|----------------|-----:|---|-----:|
|all            |       |extractive_match|0.4333|±  | 0.092|
|custom:aime24:0|      1|extractive_match|0.4333|±  | 0.092|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 44.87ba/s]
+ set +x
---- 2025-08-22T23:42:00+00:00 RUN END ----
---- 2025-08-22T23:43:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:43:11.712612727 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:43:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:44:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:44:32.731899131 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:45:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:46:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:46:09.942222977 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:46:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:47:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:47:59.781129977 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:48:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:50:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:50:30.605478579 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:51:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:54:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:54:21.821744743 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:54:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:00:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:00:33.756887131 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:01:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:06:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:06:49.800744753 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:07:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:12:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:12:59.769429209 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:13:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:19:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:19:09.650810617 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:19:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:25:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:25:18.565407213 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:25:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:31:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:31:26.674940205 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:32:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:37:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:37:37.025519115 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:38:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:43:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:43:55.899797001 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:44:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:50:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:50:06.846811098 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:50:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:56:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:56:17.884820009 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:56:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:02:31+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:02:34.785437359 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:03:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:08:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:08:43.796874575 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:09:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:15:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:15:05.768255582 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:15:43 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:21:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:21:17.885662010 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:21:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:27:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:27:39.737376308 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:28:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:33:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:33:48.667074652 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:34:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:40:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:40:08.852735429 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:40:46 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:46:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:46:35.816583211 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:47:13 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:52:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:52:58.559816200 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:53:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:59:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:59:15.574516869 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:59:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:05:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:05:35.581639481 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:06:13 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:11:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:11:45.803348396 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:12:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:18:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:18:03.796524278 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:18:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:24:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:24:16.769961108 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:24:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:30:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:30:28.719831437 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:31:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:36:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:36:46.762351730 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:37:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:42:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:43:01.792198162 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:43:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:49:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:49:14.858407317 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:49:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:55:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:55:27.815451933 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:56:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:01:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:01:48.710129697 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:02:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:08:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:08:06.724126124 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:08:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:14:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:14:16.897822826 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:14:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:20:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:20:26.789371203 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:21:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:26:44+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:26:46.704574510 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:27:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:33:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:33:06.862156140 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:33:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:39:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:39:28.788508340 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:40:06 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:45:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:45:41.710879478 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:46:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:51:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:51:55.787464811 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:52:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:58:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:58:12.734467592 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:58:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:04:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:04:20.686199303 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:04:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:10:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:10:31.820884667 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:11:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:16:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:16:52.743125992 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:17:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:23:01+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:23:04.848156033 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:23:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:29:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:29:13.772908518 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:29:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:35:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:35:31.748174832 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:36:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:41:50+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:41:53.819377884 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:42:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:48:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:48:08.776061992 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:48:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:54:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:54:27.701791887 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:55:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:00:44+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:00:46.698713527 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:01:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:06:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:06:56.819069298 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:07:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:13:04+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:13:07.831964523 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:13:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:19:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:19:24.804255512 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:20:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:25:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:25:46.747635779 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:26:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:32:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:32:03.872600583 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:32:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:38:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:38:22.790795709 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:39:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:44:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:44:40.766579394 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:45:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:50:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:50:51.813965622 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:51:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:56:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:57:02.726930628 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:57:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:03:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:03:20.690583062 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:03:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:09:40+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:09:43.835831903 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:10:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:16:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:16:03.765245280 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:16:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:22:17+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:22:20.854882665 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:22:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:28:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:28:36.853074571 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:29:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:34:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:34:45.794924435 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:35:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:40:53+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:40:56.789255637 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:41:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:47:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:47:11.789799094 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:47:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:53:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:53:26.704918816 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:54:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:59:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:59:45.756075147 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:00:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:05:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:05:54.790729008 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:06:32 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:12:04+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:12:07.812112570 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:12:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:18:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:18:21.733620310 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:18:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:24:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:24:35.814257653 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:25:13 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:30:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:30:55.827189094 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:31:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:37:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:37:05.808501093 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:37:43 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:43:12+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:43:14.686554840 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:43:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:49:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:49:30.761824344 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:50:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:55:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:55:47.669693682 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:56:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:02:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:02:04.586658697 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:02:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:08:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:08:15.578192802 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:08:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:14:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:14:32.576716162 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:15:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:20:42+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:20:45.861876024 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:21:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:26:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:26:55.843909604 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:27:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:33:15+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:33:18.823251352 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:33:56 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:39:35+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:39:38.796621669 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:40:16 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:45:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:45:50.734360675 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:46:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:52:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:52:13.922523333 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:52:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:58:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:58:27.739320219 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:59:05 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:04:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:04:46.810798539 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:05:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:10:54+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:10:57.724591355 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:11:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:17:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:17:17.859964899 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:17:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:23:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:23:28.808321536 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:24:06 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:29:41+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:29:44.931877957 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:30:22 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:35:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:35:58.799304581 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:36:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:42:12+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:42:15.799328032 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:42:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:48:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:48:26.814293433 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:49:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:54:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:54:41.784435618 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:55:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:00:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:00:58.849506910 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:01:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:07:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:07:17.781578924 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:07:55 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:13:25+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:13:28.739067601 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:14:06 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 4 --pipeline_parallel_size 1 --data_parallel_size 1
