---- 2025-08-22T09:01:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 09:02:06.224957738 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:03:18 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 09:04:22.879115276 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:05:07 [__init__.py:244] Automatically detected platform cuda.
[I822 09:05:19.619004479 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619012223 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619015890 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619022562 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619007021 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619005037 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619021506 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 09:05:19.619710252 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 09:06:22 [__init__.py:244] Automatically detected platform cuda.
[I822 09:06:44.566538649 TCPStore.cpp:274] [c10d - debug] The server has started on port = 57939.
[I822 09:06:44.566557891 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 09:06:44.566682247 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.569070344 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=124, addr=[localhost]:46388, remote=[localhost]:57939).
[I822 09:06:44.572765394 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.871668470 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.874230312 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46404, remote=[localhost]:57939).
[I822 09:06:44.878205942 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.884817192 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.884832677 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:06:44.889826454 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 09:06:44.889836303 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.952218151 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.954545025 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46418, remote=[localhost]:57939).
[I822 09:06:44.958396155 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.959298116 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.961251232 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.963318855 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.961442475 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46420, remote=[localhost]:57939).
[I822 09:06:44.963915559 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.966297370 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.963509736 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46432, remote=[localhost]:57939).
[I822 09:06:44.966406354 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.966803318 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 57939).
[I822 09:06:44.965477010 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46446, remote=[localhost]:57939).
[I822 09:06:44.968718383 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.967830745 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46458, remote=[localhost]:57939).
[I822 09:06:44.970569477 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.968374304 socket.cpp:946] [c10d] The client socket has connected to [localhost]:57939 on SocketImpl(fd=116, addr=[localhost]:46468, remote=[localhost]:57939).
[I822 09:06:44.971024003 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:57939
[I822 09:06:44.992104409 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992116115 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.992129870 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992132763 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992140455 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.992142953 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.992161667 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992175678 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.992176502 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992181470 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.992187043 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.992191880 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:06:44.992500798 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 09:06:44.992510020 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:06:44.992509782 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank7]:[I822 09:06:44.992515992 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:06:44.992522175 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 09:06:44.992528379 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:06:44.992552666 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 09:06:44.992561081 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:06:44.992580111 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 09:06:44.992584286 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 09:06:44.992586205 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:06:44.992590808 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 09:06:44.002161175 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 09:06:44.002174669 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:06:44.002479436 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 09:06:44.002487566 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:06:44.130501070 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank7]:[I822 09:06:44.130511319 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:06:44.130515752 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 09:06:44.130518799 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 09:06:44.130519223 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 09:06:44.130522867 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:06:44.130522018 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 09:06:44.130526232 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:06:44.130523723 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 09:06:44.130526817 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:06:44.130529857 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:06:44.130531421 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:06:44.130541275 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 09:06:44.130544300 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 09:06:44.130550526 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:06:44.130551501 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:118:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853605::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755853605::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 09:08:16.336315815 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 09:08:16.336350936 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:08:16.336361174 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank2]:[I822 09:08:16.336366017 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank1]:[I822 09:08:16.336384947 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:08:16.336389119 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:08:16.336417127 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank4]:[I822 09:08:16.336440509 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:08:16.336428823 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank5]:[I822 09:08:16.336431630 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank3]:[I822 09:08:16.336453028 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:08:16.336459957 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:08:16.336472777 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank7]:[I822 09:08:16.336480992 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank6]:[I822 09:08:16.336499459 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:08:16.336507199 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:08:16.337798917 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank5]:[I822 09:08:16.337801024 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank4]:[I822 09:08:16.337805668 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank5]:[I822 09:08:16.337808870 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:08:16.337810429 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:08:16.337813936 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:08:16.337815805 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 35
[rank3]:[I822 09:08:16.337816480 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank0]:[I822 09:08:16.337818549 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank7]:[I822 09:08:16.337822775 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:08:16.337819638 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank3]:[I822 09:08:16.337823842 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:08:16.337826550 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:08:16.337828074 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:08:16.337864814 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank6]:[I822 09:08:16.337876469 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 09:08:16.338844283 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank7]:[I822 09:08:16.338855024 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 09:08:16.338886532 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank6]:[I822 09:08:16.338896752 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 09:08:16.338912451 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank5]:[I822 09:08:16.338926276 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 09:08:16.338931513 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank4]:[I822 09:08:16.338938258 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 09:08:16.338957273 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank2]:[I822 09:08:16.338960811 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank3]:[I822 09:08:16.338964315 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 09:08:16.338967551 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 09:08:16.338971767 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 09:08:16.338978677 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 09:08:16.339034386 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank0]:[I822 09:08:16.339045252 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=5 pid=117)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=6 pid=118)[0;0m [1;36m(VllmWorker rank=3 pid=115)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=7 pid=119)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:35<01:47, 35.97s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:46<00:41, 20.77s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:25<00:29, 29.35s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 30.56s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.52s/it]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank5]:[I822 09:12:22.100649904 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 5] Using non-blocking mode: 0
[rank1]:[I822 09:12:22.100658223 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank2]:[I822 09:12:22.100655425 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank7]:[I822 09:12:22.100667810 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 7] Using non-blocking mode: 0
[rank6]:[I822 09:12:22.100669245 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 6] Using non-blocking mode: 0
[rank3]:[I822 09:12:22.100704250 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank4]:[I822 09:12:22.101563351 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 4] Using non-blocking mode: 0
[rank0]:[I822 09:12:22.101570719 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank0]:[I822 09:12:22.101788673 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.025264 ms
[rank0]:[I822 09:12:22.101818804 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank1]:[I822 09:12:22.101941436 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 1.24731 ms
[rank7]:[I822 09:12:22.101951748 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL broadcast unique ID through store took 1.25182 ms
[rank1]:[I822 09:12:22.101970628 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank7]:[I822 09:12:22.101973255 NCCLUtils.cpp:75] Rank 7: creating NCCL communicator with mode: blocking
[rank6]:[I822 09:12:22.101968000 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL broadcast unique ID through store took 1.26035 ms
[rank6]:[I822 09:12:22.101990283 NCCLUtils.cpp:75] Rank 6: creating NCCL communicator with mode: blocking
[rank5]:[I822 09:12:22.101995563 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL broadcast unique ID through store took 1.28455 ms
[rank2]:[I822 09:12:22.101988413 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 1.28582 ms
[rank4]:[I822 09:12:22.102009659 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL broadcast unique ID through store took 0.41557 ms
[rank5]:[I822 09:12:22.102022906 NCCLUtils.cpp:75] Rank 5: creating NCCL communicator with mode: blocking
[rank4]:[I822 09:12:22.102030613 NCCLUtils.cpp:75] Rank 4: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:12:22.102033992 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank3]:[I822 09:12:22.102034740 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 1.2849 ms
[rank3]:[I822 09:12:22.102086182 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank2]:[I822 09:12:23.048449403 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank3]:[I822 09:12:23.048709778 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank5]:[I822 09:12:23.048810547 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 5] NCCL_DEBUG: WARN
[rank7]:[I822 09:12:23.049130145 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 7] NCCL_DEBUG: WARN
[rank0]:[I822 09:12:23.049176338 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank6]:[I822 09:12:23.049447197 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 6] NCCL_DEBUG: WARN
[rank1]:[I822 09:12:23.049673724 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank4]:[I822 09:12:23.049682688 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 4] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:37,  1.75it/s]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:01<00:36,  1.77it/s]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:01<00:35,  1.78it/s]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:02<00:35,  1.79it/s]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:02<00:34,  1.79it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:03<00:34,  1.79it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:03<00:33,  1.80it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:04<00:32,  1.81it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:04<00:31,  1.82it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:05<00:31,  1.83it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:06<00:30,  1.83it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:06<00:30,  1.83it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:07<00:29,  1.83it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:07<00:29,  1.82it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:08<00:28,  1.81it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:08<00:28,  1.80it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:09<00:28,  1.74it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:10<00:29,  1.65it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:10<00:29,  1.60it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:11<00:29,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:12<00:29,  1.54it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:12<00:29,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:13<00:28,  1.53it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:14<00:26,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:14<00:25,  1.65it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:15<00:24,  1.68it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:15<00:23,  1.71it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:16<00:22,  1.73it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:16<00:21,  1.74it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:17<00:21,  1.75it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:17<00:20,  1.75it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:18<00:19,  1.76it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:19<00:19,  1.76it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:19<00:18,  1.76it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:20<00:18,  1.76it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:20<00:17,  1.76it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:21<00:17,  1.76it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:21<00:16,  1.76it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:22<00:15,  1.76it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:23<00:15,  1.76it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:23<00:14,  1.75it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:24<00:14,  1.75it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:24<00:13,  1.76it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:25<00:13,  1.77it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:25<00:12,  1.77it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:26<00:11,  1.78it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:27<00:11,  1.78it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:27<00:10,  1.77it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:28<00:10,  1.77it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:28<00:09,  1.76it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:29<00:09,  1.76it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:29<00:08,  1.74it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:30<00:08,  1.74it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:31<00:07,  1.74it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:31<00:06,  1.74it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:32<00:06,  1.74it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:32<00:05,  1.74it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:33<00:05,  1.74it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:33<00:04,  1.74it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:34<00:04,  1.74it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:35<00:03,  1.73it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:35<00:02,  1.73it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:36<00:02,  1.73it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:36<00:01,  1.73it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:37<00:01,  1.76it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:37<00:00,  1.76it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:38<00:00,  1.76it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:38<00:00,  1.74it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 30/30 [00:00<00:00, 2920.82it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|▎         | 1/30 [00:23<11:10, 23.13s/it, est. speed input: 5.62 toks/s, output: 88.96 toks/s][A
Processed prompts:   7%|▋         | 2/30 [00:29<06:09, 13.20s/it, est. speed input: 10.58 toks/s, output: 158.13 toks/s][A
Processed prompts:  10%|█         | 3/30 [00:31<03:34,  7.96s/it, est. speed input: 16.66 toks/s, output: 237.30 toks/s][A
Processed prompts:  13%|█▎        | 4/30 [00:32<02:16,  5.25s/it, est. speed input: 22.79 toks/s, output: 316.87 toks/s][A
Processed prompts:  17%|█▋        | 5/30 [00:37<02:13,  5.35s/it, est. speed input: 23.09 toks/s, output: 357.65 toks/s][A
Processed prompts:  20%|██        | 6/30 [00:41<01:52,  4.68s/it, est. speed input: 24.69 toks/s, output: 414.95 toks/s][A
Processed prompts:  23%|██▎       | 7/30 [00:44<01:36,  4.20s/it, est. speed input: 27.99 toks/s, output: 471.27 toks/s][A
Processed prompts:  27%|██▋       | 8/30 [00:47<01:24,  3.85s/it, est. speed input: 30.84 toks/s, output: 526.64 toks/s][A
Processed prompts:  30%|███       | 9/30 [01:12<03:39, 10.43s/it, est. speed input: 22.20 toks/s, output: 429.67 toks/s][A
Processed prompts:  33%|███▎      | 10/30 [01:16<02:49,  8.49s/it, est. speed input: 23.34 toks/s, output: 490.53 toks/s][A
Processed prompts:  37%|███▋      | 11/30 [01:35<03:41, 11.68s/it, est. speed input: 20.12 toks/s, output: 476.34 toks/s][A
Processed prompts:  40%|████      | 12/30 [01:35<02:28,  8.27s/it, est. speed input: 22.00 toks/s, output: 557.15 toks/s][A
Processed prompts:  43%|████▎     | 13/30 [01:55<03:19, 11.73s/it, est. speed input: 19.64 toks/s, output: 544.30 toks/s][A
Processed prompts:  47%|████▋     | 14/30 [01:55<02:12,  8.30s/it, est. speed input: 21.01 toks/s, output: 624.55 toks/s][A
Processed prompts:  50%|█████     | 15/30 [01:56<01:28,  5.89s/it, est. speed input: 23.37 toks/s, output: 705.02 toks/s][A
Processed prompts:  53%|█████▎    | 16/30 [01:57<01:01,  4.41s/it, est. speed input: 24.30 toks/s, output: 781.20 toks/s][A
Processed prompts:  57%|█████▋    | 17/30 [02:39<03:24, 15.75s/it, est. speed input: 19.32 toks/s, output: 656.23 toks/s][A
Processed prompts:  60%|██████    | 18/30 [02:41<02:20, 11.74s/it, est. speed input: 20.01 toks/s, output: 728.05 toks/s][A
Processed prompts:  63%|██████▎   | 19/30 [02:48<01:52, 10.27s/it, est. speed input: 20.59 toks/s, output: 780.10 toks/s][A
Processed prompts:  67%|██████▋   | 20/30 [03:53<04:26, 26.62s/it, est. speed input: 16.92 toks/s, output: 644.11 toks/s][A
Processed prompts:  70%|███████   | 21/30 [07:16<11:56, 79.61s/it, est. speed input: 9.64 toks/s, output: 419.37 toks/s] [A
Processed prompts: 100%|██████████| 30/30 [07:16<00:00, 79.61s/it, est. speed input: 13.06 toks/s, output: 1095.05 toks/s][AProcessed prompts: 100%|██████████| 30/30 [07:16<00:00, 14.55s/it, est. speed input: 13.06 toks/s, output: 1095.05 toks/s]
Splits: 100%|██████████| 1/1 [07:16<00:00, 436.66s/it]Splits: 100%|██████████| 1/1 [07:16<00:00, 436.66s/it]
[rank5]:[I822 09:20:28.572992381 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 5] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.573065608 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 4] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:20:28.573103050 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.573109820 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 6] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.573112646 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.573119462 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 7] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.573916408 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.573966205 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.575413856 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 5] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.575462089 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 4] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:20:28.575507391 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.575504403 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.575516915 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.575521303 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 6] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.575523388 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 7] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.575584898 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.575603452 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:20:28.575626861 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.575652175 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.575648952 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.576324667 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.576357459 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.576422813 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.576472304 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.866988713 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank3]:[I822 09:20:28.878431978 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank0]:[I822 09:20:28.879412706 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank2]:[I822 09:20:28.890097426 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank6]:[I822 09:20:28.899590216 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 6] Destroy complete.
[rank7]:[I822 09:20:28.903366506 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 7] Destroy complete.
[rank4]:[I822 09:20:28.905937985 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 4] Destroy complete.
[rank5]:[I822 09:20:28.907249819 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 5] Destroy complete.
[rank0]:[I822 09:20:28.935148721 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:20:28.935176168 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.935235171 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:20:28.935240573 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.935295843 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:20:28.935301501 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 09:20:28.935324230 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:20:28.935343924 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:20:28.945262262 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:20:28.945293880 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:20:28.945368815 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.945375111 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.945455415 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.945462140 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 09:20:28.945495569 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:20:28.945515118 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.953202644 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:20:28.953213153 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.953257576 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:20:28.953263782 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 21 Rank 0] Destroy complete.
[rank0]:[I822 09:20:28.953279600 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:20:28.953296930 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:20:28.962675364 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 23 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.962685216 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 23 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.962741762 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 23 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.962746376 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 23 Rank 0] Destroy complete.
[rank1]:[I822 09:20:28.962760215 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:20:28.962781849 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.963376846 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.963400564 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.963454145 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.963459656 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:20:28.963529608 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.963535404 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 09:20:28.963559313 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.963575175 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.969382865 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:20:28.969393188 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.969441000 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:20:28.969445002 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 0] Destroy complete.
[rank3]:[I822 09:20:28.969546530 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:20:28.969587110 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:20:28.969649438 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.969655804 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.969731171 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.969737511 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 09:20:28.969768712 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:20:28.969788019 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.973499367 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:20:28.973522210 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.973571789 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.973577804 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.973646446 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:20:28.973652996 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 13 Rank 0] Destroy complete.
[rank4]:[I822 09:20:28.973672551 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:20:28.973707749 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.974071506 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 25 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.974082623 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 25 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:20:28.974141964 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 25 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.974146201 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 25 Rank 0] Destroy complete.
[rank2]:[I822 09:20:28.974159840 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.974187829 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 09:20:28.975529445 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.975538230 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.975583829 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.975588212 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 1] Destroy complete.
[rank5]:[I822 09:20:28.979681189 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:20:28.979708872 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:20:28.979765530 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.979771533 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:20:28.979835744 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:20:28.979842086 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 15 Rank 0] Destroy complete.
[rank5]:[I822 09:20:28.979867960 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:20:28.979888787 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:20:28.983412939 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 27 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.983424276 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 27 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.983475516 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 27 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.983479767 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 27 Rank 0] Destroy complete.
[rank3]:[I822 09:20:28.983495432 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:20:28.983511442 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:20:28.983678163 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:20:28.983703853 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:20:28.983774708 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.983780062 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.983793156 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:20:28.983820336 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:20:28.983844753 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.983849519 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 17 Rank 0] Destroy complete.
[rank6]:[I822 09:20:28.983880756 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:20:28.983897888 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.983902418 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.983901619 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:20:28.983964901 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.983973570 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 19 Rank 0] Destroy complete.
[rank7]:[I822 09:20:28.984017020 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:20:28.984040610 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.991487952 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.991495817 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:20:28.991551253 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.991555729 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 2] Destroy complete.
[rank5]:[I822 09:20:28.993289104 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 31 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.993298565 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 31 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:20:28.993365727 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 31 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:20:28.993370766 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 31 Rank 0] Destroy complete.
[rank5]:[I822 09:20:28.993387030 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:20:28.993405544 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.993595739 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 29 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.993607243 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 29 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.993649243 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 29 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:20:28.993651684 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 29 Rank 0] Destroy complete.
[rank4]:[I822 09:20:28.993666073 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:20:28.993690635 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:20:28.996379859 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 33 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.996391840 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 33 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.996450824 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 33 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.996455702 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 33 Rank 0] Destroy complete.
[rank6]:[I822 09:20:28.996475287 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:20:28.996497671 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:20:28.003594813 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.003606515 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.003654911 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.003660872 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 3] Destroy complete.
[rank5]:[I822 09:20:28.003805154 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.003816664 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:20:28.003875072 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:20:28.003880379 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 5] Destroy complete.
[rank7]:[I822 09:20:28.004170940 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 35 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.004179219 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 35 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.004234577 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 35 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.004239212 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 35 Rank 0] Destroy complete.
[rank7]:[I822 09:20:28.004253737 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:20:28.004269945 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.007907117 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.007917746 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.007961595 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:20:28.007965870 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 4] Destroy complete.
[rank6]:[I822 09:20:28.010832586 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.010841579 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.010885557 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.010889600 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 6] Destroy complete.
[rank7]:[I822 09:20:28.020494228 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.020503572 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.020560910 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.020565456 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 7] Destroy complete.
[rank0]:[I822 09:20:28.055934311 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:20:28.055955885 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.055979392 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 09:20:28.055982164 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.056027408 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL destructor entered.
[rank0]:[I822 09:20:28.056049360 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.056052718 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.056054065 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 09:20:28.056072098 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 09:20:28.056073644 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.056076934 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.056084761 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.056081384 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:20:28.056086547 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.056104101 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:20:28.056110301 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.056110189 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 09:20:28.056126373 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 09:20:28.056132767 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.056136205 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.056137484 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.056142469 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.056143817 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.056147837 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank4]:[I822 09:20:28.056150098 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.056153856 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 4] Operations flushed, joining watchdog thread.
[rank1]:[I822 09:20:28.056165648 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.056188468 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.056188970 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:20:28.056190527 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.056198118 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank4]:[I822 09:20:28.056197288 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.056197778 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:20:28.056201457 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 4] Destroy complete.
[rank3]:[I822 09:20:28.056202583 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank5]:[I822 09:20:28.056211407 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.056217014 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.056219187 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank3]:[I822 09:20:28.056223575 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank2]:[I822 09:20:28.056231625 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:20:28.056238283 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.056242435 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 5] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.056254994 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 09:20:28.056276286 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:20:28.056301731 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:20:28.056308659 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 5] Destroy complete.
[rank5]:[I822 09:20:28.056329426 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 09:20:28.056347212 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 09:20:28.066243467 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:20:28.066269248 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:20:28.066285961 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:20:28.066290229 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.066294307 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 6] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.066303411 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:20:28.066327940 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.066337912 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 7] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.066343097 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.066347160 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 6] Destroy complete.
[rank6]:[I822 09:20:28.066364337 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 09:20:28.066378187 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:20:28.066395912 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.066399939 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 7] Destroy complete.
[rank7]:[I822 09:20:28.066414967 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 09:20:28.066447451 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 09:20:28.099382579 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.099384393 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 09:20:28.099391149 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.099394801 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 09:20:28.099438010 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 09:20:28.099444158 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[rank1]:[I822 09:20:28.099444961 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 09:20:28.099447824 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 09:20:28.099483756 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 09:20:28.099495368 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 09:20:28.099503503 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:20:28.099515904 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:20:28.099547266 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 09:20:28.099624538 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 09:20:28.099647940 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 09:20:28.099654414 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 09:20:28.099657256 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 09:20:28.099660805 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099663587 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099665503 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099667761 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099669722 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099671850 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099673751 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 09:20:28.099726904 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 09:20:28.099730970 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099733025 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099734920 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099736814 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099738660 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099740536 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099742494 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 09:20:28.099744795 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 09:20:28.099771425 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank7]:[W822 09:20:28.110715133 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=116, addr=[localhost]:46446, remote=[localhost]:57939): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fcf2fb785e8 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7fcf18e5aafe in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7fcf18e5ce40 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7fcf18e5d74a in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7fcf18e571a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7fced60509a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbbf4 (0x7fcec5fefbf4 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/bin/../lib/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7fcf30e75ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fcf30f07850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[W822 09:20:28.110716643 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=116, addr=[localhost]:46468, remote=[localhost]:57939): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f6b849785e8 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7f6b6dc5aafe in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7f6b6dc5ce40 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7f6b6dc5d74a in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f6b6dc571a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f6b2ae509a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbbf4 (0x7f6b1adefbf4 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/bin/../lib/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f6b85c4aac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f6b85cdc850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[W822 09:20:28.110722797 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=116, addr=[localhost]:46458, remote=[localhost]:57939): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f9e739785e8 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7f9e5c85aafe in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7f9e5c85ce40 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7f9e5c85d74a in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f9e5c8571a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f9e19a509a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbbf4 (0x7f9e099efbf4 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/bin/../lib/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f9e74904ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f9e74996850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[W822 09:20:28.110717926 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=116, addr=[localhost]:46418, remote=[localhost]:57939): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f6cea1785e8 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7f6cd305aafe in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7f6cd305ce40 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7f6cd305d74a in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f6cd30571a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f6c902509a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbbf4 (0x7f6c801efbf4 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/bin/../lib/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7f6ceb18bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7f6ceb21d850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank6]:[W822 09:20:28.110724017 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=116, addr=[localhost]:46432, remote=[localhost]:57939): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fcab99785e8 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7fcaa285aafe in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7fcaa285ce40 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7fcaa285d74a in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7fcaa28571a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7fca5fa509a9 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xdbbf4 (0x7fca4f9efbf4 in /code-fsx/yibiaoy-sandbox/miniconda3/envs/soberplus/bin/../lib/libstdc++.so.6)
frame #7: <unknown function> + 0x94ac3 (0x7fcaba985ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x126850 (0x7fcabaa17850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[I822 09:20:28.119503946 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 09:20:28.119517867 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank3]:[I822 09:20:28.119532657 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 09:20:28.119541067 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 09:20:28.119564250 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 09:20:28.119568679 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[rank3]:[I822 09:20:28.119599088 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 09:20:28.119607096 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 09:20:28.119611007 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 09:20:28.119645728 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[rank7]:[W822 09:20:28.120201544 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 7] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[rank4]:[W822 09:20:28.120206155 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 4] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[rank6]:[W822 09:20:28.120206818 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 6] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[W822 09:20:28.120205148 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 3] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[W822 09:20:28.120211398 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[I822 09:20:28.120256707 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:20:28.120262749 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 09:20:28.129593625 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 09:20:28.129604715 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 09:20:28.129647821 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 09:20:28.129652118 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 4] Destroy complete.
[I822 09:20:28.129687923 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[I822 09:20:28.129706512 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 09:20:28.140199983 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 09:20:28.140214645 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 5] Operations flushed, joining watchdog thread.
[rank6]:[I822 09:20:28.140245576 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 09:20:28.140255072 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 6] Operations flushed, joining watchdog thread.
[rank5]:[I822 09:20:28.140278514 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 09:20:28.140286290 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 5] Destroy complete.
[rank6]:[I822 09:20:28.140305641 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 09:20:28.140309152 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 6] Destroy complete.
[I822 09:20:28.140343322 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[I822 09:20:28.140350538 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[I822 09:20:28.140377852 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[I822 09:20:28.140378150 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 09:20:28.149636003 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 09:20:28.149648769 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 09:20:28.149703077 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 09:20:28.149707701 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 7] Destroy complete.
[I822 09:20:28.149750343 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL destructor entered.
[I822 09:20:28.149771407 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value |   |Stderr|
|---------------|------:|----------------|-----:|---|-----:|
|all            |       |extractive_match|0.5667|±  | 0.092|
|custom:aime24:0|      1|extractive_match|0.5667|±  | 0.092|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 29.15ba/s]
+ set +x
---- 2025-08-22T09:20:37+00:00 RUN END ----
---- 2025-08-22T23:28:15+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:28:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
Traceback (most recent call last):
  File "/code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py", line 21, in <module>
    from lm_eval import evaluator
ModuleNotFoundError: No module named 'lm_eval'
---- 2025-08-22T23:29:12+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:29:14.391854487 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:30:02 [__init__.py:244] Automatically detected platform cuda.
Using seed: 0
[I822 23:31:00.171495975 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:31:45 [__init__.py:244] Automatically detected platform cuda.
[I822 23:31:57.255991983 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259432721 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259433348 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259439375 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259440469 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259445124 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.259450490 debug.cpp:50] [c10d] The debug level is set to INFO.
[I822 23:31:57.260169293 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
INFO 08-22 23:33:00 [__init__.py:244] Automatically detected platform cuda.
[I822 23:33:19.280483832 TCPStore.cpp:274] [c10d - debug] The server has started on port = 36757.
[I822 23:33:19.280506005 TCPStoreLibUvBackend.cpp:1178] [c10d - debug] Uv main loop running
[I822 23:33:19.280601378 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:19.282322676 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=124, addr=[localhost]:56174, remote=[localhost]:36757).
[I822 23:33:19.284876785 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.557593527 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.558125696 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.558276021 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.558286971 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.560577382 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.559372207 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56176, remote=[localhost]:36757).
[I822 23:33:20.561936741 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.559701555 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56184, remote=[localhost]:36757).
[I822 23:33:20.562150447 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.559730731 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56198, remote=[localhost]:36757).
[I822 23:33:20.562229719 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.559877818 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56208, remote=[localhost]:36757).
[I822 23:33:20.562579658 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.562590759 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.563034208 socket.cpp:776] [c10d - debug] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 36757).
[I822 23:33:20.562183583 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56218, remote=[localhost]:36757).
[I822 23:33:20.564741195 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.563953883 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56234, remote=[localhost]:36757).
[I822 23:33:20.566461891 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.564563015 socket.cpp:946] [c10d] The client socket has connected to [localhost]:36757 on SocketImpl(fd=116, addr=[localhost]:56244, remote=[localhost]:36757).
[I822 23:33:20.566979810 TCPStore.cpp:319] [c10d - debug] TCP client connected to host 127.0.0.1:36757
[I822 23:33:20.595878265 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595880594 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595881984 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595884542 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595890222 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595891379 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595891366 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595893634 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595892493 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595902953 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595901236 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595911548 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.595913636 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.595922542 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:20.596242347 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 23:33:20.596245005 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank5]:[I822 23:33:20.596246427 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank3]:[I822 23:33:20.596248684 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank1]:[I822 23:33:20.596251211 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:33:20.596252563 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:20.596252967 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:20.596254587 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:33:20.596253929 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank4]:[I822 23:33:20.596260155 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:33:20.596270095 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank6]:[I822 23:33:20.596276402 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:20.596365428 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank2]:[I822 23:33:20.596377422 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[I822 23:33:20.605975768 ProcessGroupNCCL.cpp:978] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 0
[I822 23:33:20.605989049 ProcessGroupNCCL.cpp:987] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:20.606289673 ProcessGroupNCCL.cpp:978] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 1
[rank0]:[I822 23:33:20.606298007 ProcessGroupNCCL.cpp:987] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:33:20.659490523 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank0]:[I822 23:33:20.659500506 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:33:20.659601452 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank7]:[I822 23:33:20.659610791 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:33:20.659681291 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank1]:[I822 23:33:20.659690931 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:33:20.659720778 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank2]:[I822 23:33:20.659730024 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:33:20.659755469 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank6]:[I822 23:33:20.659765454 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:33:20.659795772 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank3]:[I822 23:33:20.659805136 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:33:20.659823016 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank5]:[I822 23:33:20.659832870 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:33:20.659838929 ProcessGroupNCCL.cpp:978] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 3
[rank4]:[I822 23:33:20.659845759 ProcessGroupNCCL.cpp:987] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
NCCL version 2.26.2+cuda12.2
libfabric:113:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:114:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:114:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:118:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:119:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:116:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:112:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:115:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:117:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:113:1755905600::core:core:cuda_gdrcopy_hmem_init():201<warn> gdr_open failed!
libfabric:118:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:119:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:112:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:116:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:115:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:113:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
libfabric:117:1755905600::core:core:cuda_hmem_init():791<warn> gdrcopy initialization failed! gdrcopy will not be used.
[rank0]:[I822 23:34:52.646763242 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 5
[rank0]:[I822 23:34:52.646793954 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:52.646830138 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 7
[rank1]:[I822 23:34:52.646862349 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:52.646875256 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 11
[rank4]:[I822 23:34:52.646880775 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 13
[rank3]:[I822 23:34:52.646900593 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:52.646902970 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:52.646905027 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 9
[rank2]:[I822 23:34:52.646926566 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:52.646950139 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 19
[rank5]:[I822 23:34:52.646960257 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 15
[rank7]:[I822 23:34:52.646972492 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:52.646980644 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:52.646964379 ProcessGroupNCCL.cpp:978] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 17
[rank6]:[I822 23:34:52.646991284 ProcessGroupNCCL.cpp:987] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:34:52.647897602 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 21
[rank0]:[I822 23:34:52.647908428 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:52.648222195 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 35
[rank7]:[I822 23:34:52.648233663 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:52.648249115 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 27
[rank3]:[I822 23:34:52.648259749 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:52.648262543 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 31
[rank5]:[I822 23:34:52.648270830 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:52.648275219 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 29
[rank4]:[I822 23:34:52.648282670 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:52.648287172 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 23
[rank1]:[I822 23:34:52.648301632 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:52.648316425 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 25
[rank2]:[I822 23:34:52.648330125 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:52.648342849 ProcessGroupNCCL.cpp:978] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL initialization options: size: 1, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 33
[rank6]:[I822 23:34:52.648358513 ProcessGroupNCCL.cpp:987] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank0]:[I822 23:34:52.648773680 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL initialization options: size: 8, global rank: 0, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank0]:[I822 23:34:52.648783429 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank7]:[I822 23:34:52.649207582 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL initialization options: size: 8, global rank: 7, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank7]:[I822 23:34:52.649216690 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank5]:[I822 23:34:52.649260983 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL initialization options: size: 8, global rank: 5, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank5]:[I822 23:34:52.649270627 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank4]:[I822 23:34:52.649360826 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL initialization options: size: 8, global rank: 4, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank4]:[I822 23:34:52.649370628 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank3]:[I822 23:34:52.649369453 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL initialization options: size: 8, global rank: 3, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank3]:[I822 23:34:52.649376789 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank6]:[I822 23:34:52.649405566 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL initialization options: size: 8, global rank: 6, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank6]:[I822 23:34:52.649415763 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank2]:[I822 23:34:52.649445514 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL initialization options: size: 8, global rank: 2, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank2]:[I822 23:34:52.649455702 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[rank1]:[I822 23:34:52.649456240 ProcessGroupNCCL.cpp:978] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL initialization options: size: 8, global rank: 1, TIMEOUT(ms): 600000, USE_HIGH_PRIORITY_STREAM: 0, SPLIT_FROM: 0, SPLIT_COLOR: -2, PG Name: 37
[rank1]:[I822 23:34:52.649464867 ProcessGroupNCCL.cpp:987] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL environments: NCCL version: 2.26.2, TORCH_NCCL_ASYNC_ERROR_HANDLING: 3, TORCH_NCCL_DUMP_ON_TIMEOUT: 1, TORCH_NCCL_PROPAGATE_ERROR: 0, TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC: 60000, TORCH_NCCL_DESYNC_DEBUG: 0, TORCH_NCCL_ENABLE_TIMING: 0, TORCH_NCCL_BLOCKING_WAIT: 0, TORCH_DISTRIBUTED_DEBUG: INFO, TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0, TORCH_NCCL_ENABLE_MONITORING: 1, TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC: 480, TORCH_NCCL_TRACE_BUFFER_SIZE: 2000, TORCH_NCCL_COORD_CHECK_MILSEC: 1000, TORCH_NCCL_NAN_CHECK: 0, TORCH_NCCL_CUDA_EVENT_CACHE: 1, TORCH_NCCL_LOG_CPP_STACK_ON_UNCLEAN_SHUTDOWN: 1
[1;36m(VllmWorker rank=0 pid=112)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=3 pid=115)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=2 pid=114)[0;0m [1;36m(VllmWorker rank=1 pid=113)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=7 pid=119)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=5 pid=117)[0;0m [1;36m(VllmWorker rank=6 pid=118)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=4 pid=116)[0;0m FlashInfer version >= 0.2.3 required. Falling back to default sampling implementation.
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.15it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.76it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.42it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.30it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]
[1;36m(VllmWorker rank=0 pid=112)[0;0m 
[rank0]:[I822 23:36:52.144533835 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 0] Using non-blocking mode: 0
[rank2]:[I822 23:36:52.144545807 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 2] Using non-blocking mode: 0
[rank6]:[I822 23:36:52.144547728 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 6] Using non-blocking mode: 0
[rank4]:[I822 23:36:52.144579550 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 4] Using non-blocking mode: 0
[rank7]:[I822 23:36:52.144685770 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 7] Using non-blocking mode: 0
[rank3]:[I822 23:36:52.144764687 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 3] Using non-blocking mode: 0
[rank0]:[I822 23:36:52.144786904 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL broadcast unique ID through store took 0.027023 ms
[rank0]:[I822 23:36:52.144817065 NCCLUtils.cpp:75] Rank 0: creating NCCL communicator with mode: blocking
[rank3]:[I822 23:36:52.144983146 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL broadcast unique ID through store took 0.192234 ms
[rank6]:[I822 23:36:52.144978282 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL broadcast unique ID through store took 0.393937 ms
[rank3]:[I822 23:36:52.145021922 NCCLUtils.cpp:75] Rank 3: creating NCCL communicator with mode: blocking
[rank7]:[I822 23:36:52.145004674 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL broadcast unique ID through store took 0.285927 ms
[rank7]:[I822 23:36:52.145032100 NCCLUtils.cpp:75] Rank 7: creating NCCL communicator with mode: blocking
[rank6]:[I822 23:36:52.145032088 NCCLUtils.cpp:75] Rank 6: creating NCCL communicator with mode: blocking
[rank4]:[I822 23:36:52.145031910 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL broadcast unique ID through store took 0.420223 ms
[rank4]:[I822 23:36:52.145083060 NCCLUtils.cpp:75] Rank 4: creating NCCL communicator with mode: blocking
[rank1]:[I822 23:36:52.145182968 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 1] Using non-blocking mode: 0
[rank2]:[I822 23:36:52.145192988 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL broadcast unique ID through store took 0.621326 ms
[rank2]:[I822 23:36:52.145217653 NCCLUtils.cpp:75] Rank 2: creating NCCL communicator with mode: blocking
[rank1]:[I822 23:36:52.145323587 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL broadcast unique ID through store took 0.11111 ms
[rank1]:[I822 23:36:52.145347949 NCCLUtils.cpp:75] Rank 1: creating NCCL communicator with mode: blocking
[rank5]:[I822 23:36:52.145588452 ProcessGroupNCCL.cpp:1075] [PG ID 2 PG GUID 3 Rank 5] Using non-blocking mode: 0
[rank5]:[I822 23:36:52.145721835 ProcessGroupNCCL.cpp:2825] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL broadcast unique ID through store took 0.103639 ms
[rank5]:[I822 23:36:52.145748055 NCCLUtils.cpp:75] Rank 5: creating NCCL communicator with mode: blocking
[rank7]:[I822 23:36:53.866436711 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 7] NCCL_DEBUG: WARN
[rank1]:[I822 23:36:53.866598204 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 1] NCCL_DEBUG: WARN
[rank6]:[I822 23:36:53.866644678 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 6] NCCL_DEBUG: WARN
[rank3]:[I822 23:36:53.866735826 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 3] NCCL_DEBUG: WARN
[rank5]:[I822 23:36:53.866860178 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 5] NCCL_DEBUG: WARN
[rank0]:[I822 23:36:53.867019207 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 0] NCCL_DEBUG: WARN
[rank2]:[I822 23:36:53.867044073 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 2] NCCL_DEBUG: WARN
[rank4]:[I822 23:36:53.867064052 ProcessGroupNCCL.cpp:2858] [PG ID 2 PG GUID 3 Rank 4] NCCL_DEBUG: WARN
[1;36m(VllmWorker rank=0 pid=112)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:35,  1.88it/s]Capturing CUDA graph shapes:   3%|▎         | 2/67 [00:01<00:34,  1.90it/s]Capturing CUDA graph shapes:   4%|▍         | 3/67 [00:01<00:33,  1.91it/s]Capturing CUDA graph shapes:   6%|▌         | 4/67 [00:02<00:33,  1.90it/s]Capturing CUDA graph shapes:   7%|▋         | 5/67 [00:02<00:32,  1.89it/s]Capturing CUDA graph shapes:   9%|▉         | 6/67 [00:03<00:32,  1.89it/s]Capturing CUDA graph shapes:  10%|█         | 7/67 [00:03<00:31,  1.89it/s]Capturing CUDA graph shapes:  12%|█▏        | 8/67 [00:04<00:31,  1.88it/s]Capturing CUDA graph shapes:  13%|█▎        | 9/67 [00:04<00:30,  1.88it/s]Capturing CUDA graph shapes:  15%|█▍        | 10/67 [00:05<00:30,  1.89it/s]Capturing CUDA graph shapes:  16%|█▋        | 11/67 [00:05<00:29,  1.89it/s]Capturing CUDA graph shapes:  18%|█▊        | 12/67 [00:06<00:29,  1.89it/s]Capturing CUDA graph shapes:  19%|█▉        | 13/67 [00:06<00:28,  1.90it/s]Capturing CUDA graph shapes:  21%|██        | 14/67 [00:07<00:27,  1.91it/s]Capturing CUDA graph shapes:  22%|██▏       | 15/67 [00:07<00:27,  1.90it/s]Capturing CUDA graph shapes:  24%|██▍       | 16/67 [00:08<00:27,  1.87it/s]Capturing CUDA graph shapes:  25%|██▌       | 17/67 [00:09<00:26,  1.86it/s]Capturing CUDA graph shapes:  27%|██▋       | 18/67 [00:09<00:26,  1.86it/s]Capturing CUDA graph shapes:  28%|██▊       | 19/67 [00:10<00:25,  1.86it/s]Capturing CUDA graph shapes:  30%|██▉       | 20/67 [00:10<00:25,  1.85it/s]Capturing CUDA graph shapes:  31%|███▏      | 21/67 [00:11<00:24,  1.85it/s]Capturing CUDA graph shapes:  33%|███▎      | 22/67 [00:11<00:24,  1.85it/s]Capturing CUDA graph shapes:  34%|███▍      | 23/67 [00:12<00:23,  1.85it/s]Capturing CUDA graph shapes:  36%|███▌      | 24/67 [00:12<00:23,  1.85it/s]Capturing CUDA graph shapes:  37%|███▋      | 25/67 [00:13<00:22,  1.85it/s]Capturing CUDA graph shapes:  39%|███▉      | 26/67 [00:13<00:22,  1.85it/s]Capturing CUDA graph shapes:  40%|████      | 27/67 [00:14<00:21,  1.85it/s]Capturing CUDA graph shapes:  42%|████▏     | 28/67 [00:14<00:21,  1.84it/s]Capturing CUDA graph shapes:  43%|████▎     | 29/67 [00:15<00:20,  1.84it/s]Capturing CUDA graph shapes:  45%|████▍     | 30/67 [00:16<00:20,  1.83it/s]Capturing CUDA graph shapes:  46%|████▋     | 31/67 [00:16<00:19,  1.83it/s]Capturing CUDA graph shapes:  48%|████▊     | 32/67 [00:17<00:19,  1.83it/s]Capturing CUDA graph shapes:  49%|████▉     | 33/67 [00:17<00:18,  1.82it/s]Capturing CUDA graph shapes:  51%|█████     | 34/67 [00:18<00:18,  1.82it/s]Capturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:18<00:17,  1.81it/s]Capturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:19<00:17,  1.82it/s]Capturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:19<00:16,  1.83it/s]Capturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:20<00:15,  1.83it/s]Capturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:20<00:15,  1.84it/s]Capturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:21<00:14,  1.84it/s]Capturing CUDA graph shapes:  61%|██████    | 41/67 [00:22<00:14,  1.83it/s]Capturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:22<00:13,  1.82it/s]Capturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:23<00:13,  1.82it/s]Capturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:23<00:12,  1.83it/s]Capturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:24<00:12,  1.83it/s]Capturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:24<00:11,  1.83it/s]Capturing CUDA graph shapes:  70%|███████   | 47/67 [00:25<00:10,  1.82it/s]Capturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:25<00:10,  1.82it/s]Capturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:26<00:09,  1.81it/s]Capturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:27<00:09,  1.81it/s]Capturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:27<00:08,  1.80it/s]Capturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:28<00:08,  1.81it/s]Capturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:28<00:07,  1.80it/s]Capturing CUDA graph shapes:  81%|████████  | 54/67 [00:29<00:07,  1.82it/s]Capturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:29<00:06,  1.83it/s]Capturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:30<00:05,  1.85it/s]Capturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:30<00:05,  1.86it/s]Capturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:31<00:04,  1.86it/s]Capturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:31<00:04,  1.84it/s]Capturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:32<00:03,  1.83it/s]Capturing CUDA graph shapes:  91%|█████████ | 61/67 [00:33<00:03,  1.80it/s]Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:33<00:02,  1.80it/s]Capturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:34<00:02,  1.80it/s]Capturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:34<00:01,  1.80it/s]Capturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:35<00:01,  1.81it/s]Capturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:35<00:00,  1.81it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:36<00:00,  1.80it/s]Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:36<00:00,  1.84it/s]
If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.
Careful, the task custom|aime24 is using evaluation data to build the few shot examples.
You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.
Splits:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/30 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 30/30 [00:00<00:00, 8407.67it/s]

Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   3%|▎         | 1/30 [00:23<11:15, 23.28s/it, est. speed input: 5.58 toks/s, output: 88.88 toks/s][A
Processed prompts:   7%|▋         | 2/30 [00:27<05:32, 11.88s/it, est. speed input: 10.08 toks/s, output: 164.41 toks/s][A
Processed prompts:  10%|█         | 3/30 [00:27<02:58,  6.60s/it, est. speed input: 17.50 toks/s, output: 250.87 toks/s][A
Processed prompts:  13%|█▎        | 4/30 [00:29<02:04,  4.81s/it, est. speed input: 23.58 toks/s, output: 321.46 toks/s][A
Processed prompts:  17%|█▋        | 5/30 [00:31<01:32,  3.70s/it, est. speed input: 26.65 toks/s, output: 391.59 toks/s][A
Processed prompts:  20%|██        | 6/30 [00:36<01:43,  4.32s/it, est. speed input: 27.57 toks/s, output: 420.43 toks/s][A
Processed prompts:  23%|██▎       | 7/30 [00:43<02:00,  5.22s/it, est. speed input: 28.27 toks/s, output: 439.39 toks/s][A
Processed prompts:  27%|██▋       | 8/30 [00:58<02:57,  8.09s/it, est. speed input: 24.09 toks/s, output: 417.43 toks/s][A
Processed prompts:  30%|███       | 9/30 [01:05<02:44,  7.82s/it, est. speed input: 24.16 toks/s, output: 456.30 toks/s][A
Processed prompts:  33%|███▎      | 10/30 [01:09<02:11,  6.56s/it, est. speed input: 25.28 toks/s, output: 516.71 toks/s][A
Processed prompts:  37%|███▋      | 11/30 [01:20<02:31,  7.97s/it, est. speed input: 23.44 toks/s, output: 528.76 toks/s][A
Processed prompts:  40%|████      | 12/30 [01:20<01:42,  5.69s/it, est. speed input: 25.10 toks/s, output: 609.79 toks/s][A
Processed prompts:  43%|████▎     | 13/30 [01:25<01:32,  5.42s/it, est. speed input: 25.56 toks/s, output: 659.35 toks/s][A
Processed prompts:  47%|████▋     | 14/30 [01:39<02:06,  7.88s/it, est. speed input: 23.66 toks/s, output: 652.62 toks/s][A
Processed prompts:  50%|█████     | 15/30 [01:41<01:31,  6.13s/it, est. speed input: 25.95 toks/s, output: 722.93 toks/s][A
Processed prompts:  53%|█████▎    | 16/30 [01:43<01:10,  5.03s/it, est. speed input: 26.75 toks/s, output: 788.86 toks/s][A
Processed prompts:  57%|█████▋    | 17/30 [01:49<01:09,  5.37s/it, est. speed input: 27.64 toks/s, output: 828.10 toks/s][A
Processed prompts:  60%|██████    | 18/30 [02:07<01:49,  9.15s/it, est. speed input: 25.23 toks/s, output: 794.88 toks/s][A
Processed prompts:  63%|██████▎   | 19/30 [02:15<01:37,  8.86s/it, est. speed input: 24.84 toks/s, output: 830.37 toks/s][A
Processed prompts:  67%|██████▋   | 20/30 [02:17<01:06,  6.66s/it, est. speed input: 25.80 toks/s, output: 904.54 toks/s][A
Processed prompts:  70%|███████   | 21/30 [02:43<01:51, 12.40s/it, est. speed input: 23.15 toks/s, output: 844.75 toks/s][A
Processed prompts:  73%|███████▎  | 22/30 [03:02<01:55, 14.46s/it, est. speed input: 21.49 toks/s, output: 838.92 toks/s][A
Processed prompts:  77%|███████▋  | 23/30 [03:45<02:41, 23.13s/it, est. speed input: 19.47 toks/s, output: 762.29 toks/s][A
Processed prompts:  80%|████████  | 24/30 [04:10<02:21, 23.64s/it, est. speed input: 18.32 toks/s, output: 771.57 toks/s][A
Processed prompts:  83%|████████▎ | 25/30 [06:26<04:45, 57.15s/it, est. speed input: 12.50 toks/s, output: 585.99 toks/s][A
Processed prompts: 100%|██████████| 30/30 [06:26<00:00, 57.15s/it, est. speed input: 14.77 toks/s, output: 1010.39 toks/s][AProcessed prompts: 100%|██████████| 30/30 [06:26<00:00, 12.87s/it, est. speed input: 14.77 toks/s, output: 1010.39 toks/s]
Splits: 100%|██████████| 1/1 [06:26<00:00, 386.21s/it]Splits: 100%|██████████| 1/1 [06:26<00:00, 386.21s/it]
[rank0]:[I822 23:44:12.692468799 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.692479888 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 1] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.692492849 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 5] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.692509191 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 2] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:12.692577957 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 3] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.692580690 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 6] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:44:12.693431392 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 4] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:44:12.693431492 ProcessGroupNCCL.cpp:1413] [PG ID 2 PG GUID 3 Rank 7] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.694877990 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 5] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.694887959 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 1] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.694904202 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 2] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.694940424 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.694959983 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.694970018 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 3] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:44:12.694975962 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 6] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.694988472 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.695036226 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.695047408 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:44:12.695097468 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.695100802 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.695815837 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 7] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.695816533 ProcessGroupNCCL.cpp:1440] [PG ID 2 PG GUID 3 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.695894179 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.695910841 ProcessGroupNCCL.cpp:1454] [PG ID 2 PG GUID 3 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.130502307 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 2] Destroy complete.
[rank0]:[I822 23:44:12.146455058 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 0] Destroy complete.
[rank1]:[I822 23:44:12.151662232 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 1] Destroy complete.
[rank3]:[I822 23:44:12.164652412 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 3] Destroy complete.
[rank7]:[I822 23:44:12.173695739 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 7] Destroy complete.
[rank4]:[I822 23:44:12.177427276 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 4] Destroy complete.
[rank5]:[I822 23:44:12.178199857 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 5] Destroy complete.
[rank6]:[I822 23:44:12.179261295 ProcessGroupNCCL.cpp:1462] [PG ID 2 PG GUID 3 Rank 6] Destroy complete.
[rank0]:[I822 23:44:12.207416222 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:44:12.207440368 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:44:12.207485822 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 5 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:44:12.207491263 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 5 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.207541924 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 5 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.207547914 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 5 Rank 0] Destroy complete.
[rank0]:[I822 23:44:12.207566520 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:44:12.207599290 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 5 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.221508270 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.221531379 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.221579840 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 9 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.221585788 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 9 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.221655105 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 9 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.221661500 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 9 Rank 0] Destroy complete.
[rank2]:[I822 23:44:12.221682681 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:44:12.221683665 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.221703803 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 9 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.221708674 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.221766502 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 7 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.221776374 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 7 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.221846590 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 7 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:44:12.221852685 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 7 Rank 0] Destroy complete.
[rank1]:[I822 23:44:12.221876076 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:44:12.221894507 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 7 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:44:12.227525901 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 21 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:44:12.227537178 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 21 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.227583042 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 21 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.227587394 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 21 Rank 0] Destroy complete.
[rank0]:[I822 23:44:12.227600207 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:44:12.227615314 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 21 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:12.231720975 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:44:12.231743719 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:12.231795700 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 11 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:12.231800863 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 11 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:44:12.231873304 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 11 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.231879353 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 11 Rank 0] Destroy complete.
[rank3]:[I822 23:44:12.231901431 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:44:12.231920747 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 11 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.234306553 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 23 Rank 0] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.234318057 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 23 Rank 0] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.234371847 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 23 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:44:12.234376002 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 23 Rank 0] Destroy complete.
[rank1]:[I822 23:44:12.234390522 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:44:12.234407271 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 23 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.239310750 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 25 Rank 0] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.239322434 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 25 Rank 0] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.239379838 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 25 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.239384871 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 25 Rank 0] Destroy complete.
[rank2]:[I822 23:44:12.239398962 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.239419765 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 25 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.241800453 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:44:12.241826831 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.241889822 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 13 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:44:12.241896605 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 13 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.241965178 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 13 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:44:12.241971412 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 13 Rank 0] Destroy complete.
[rank4]:[I822 23:44:12.242001233 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:44:12.242021120 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 13 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:44:12.243526330 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:44:12.243536569 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.243588484 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.243592645 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 0] Destroy complete.
[rank3]:[I822 23:44:12.244015659 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 27 Rank 0] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:12.244024069 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 27 Rank 0] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:44:12.244080975 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 27 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.244085712 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 27 Rank 0] Destroy complete.
[rank3]:[I822 23:44:12.244099039 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:44:12.244152918 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 27 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.249065530 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.249073955 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.249120536 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:44:12.249124584 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 1] Destroy complete.
[rank6]:[I822 23:44:12.251784887 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:44:12.251808393 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:44:12.251861906 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 17 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.251867286 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 17 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.251866308 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:44:12.251894881 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:44:12.251900901 ProcessGroupNCCL.cpp:1467] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.251925719 ProcessGroupNCCL.cpp:1517] [PG ID 2 PG GUID 3 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:44:12.251931420 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 17 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:44:12.251935834 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 17 Rank 0] Destroy complete.
[rank6]:[I822 23:44:12.251955994 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.251973444 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 19 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.251975113 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 17 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:44:12.251978545 ProcessGroupNCCL.cpp:1413] [PG ID 3 PG GUID 15 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.251982835 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 15 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:44:12.251984815 ProcessGroupNCCL.cpp:1440] [PG ID 3 PG GUID 19 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.252055685 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 15 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:44:12.252065235 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 15 Rank 0] Destroy complete.
[rank7]:[I822 23:44:12.252069683 ProcessGroupNCCL.cpp:1454] [PG ID 3 PG GUID 19 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.252074785 ProcessGroupNCCL.cpp:1462] [PG ID 3 PG GUID 19 Rank 0] Destroy complete.
[rank7]:[I822 23:44:12.252096329 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:44:12.252099867 ProcessGroupNCCL.cpp:1467] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.252114876 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 19 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:44:12.252118623 ProcessGroupNCCL.cpp:1517] [PG ID 3 PG GUID 15 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.254106419 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.254118648 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.254172719 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.254177728 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 2] Destroy complete.
[rank4]:[I822 23:44:12.259289672 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 29 Rank 0] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:44:12.259302858 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 29 Rank 0] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.259354260 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 29 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:44:12.259359038 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 29 Rank 0] Destroy complete.
[rank4]:[I822 23:44:12.259383944 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:44:12.259405943 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 29 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:12.261880154 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:12.261891952 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:44:12.261952350 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.261956736 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 3] Destroy complete.
[rank5]:[I822 23:44:12.262792846 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 31 Rank 0] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.262803560 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 31 Rank 0] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.262846315 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 31 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:44:12.262853183 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 31 Rank 0] Destroy complete.
[rank5]:[I822 23:44:12.262870856 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:44:12.262890658 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 31 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:44:12.266368833 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 33 Rank 0] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.266380533 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 33 Rank 0] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:44:12.266435902 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 33 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:44:12.266440372 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 33 Rank 0] Destroy complete.
[rank6]:[I822 23:44:12.266455928 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:44:12.266471834 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 33 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.270144723 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:44:12.270153913 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.270203945 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:44:12.270208750 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 4] Destroy complete.
[rank7]:[I822 23:44:12.270859370 ProcessGroupNCCL.cpp:1413] [PG ID 4 PG GUID 35 Rank 0] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:44:12.270877651 ProcessGroupNCCL.cpp:1440] [PG ID 4 PG GUID 35 Rank 0] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:44:12.270927593 ProcessGroupNCCL.cpp:1454] [PG ID 4 PG GUID 35 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.270930138 ProcessGroupNCCL.cpp:1462] [PG ID 4 PG GUID 35 Rank 0] Destroy complete.
[rank7]:[I822 23:44:12.270964097 ProcessGroupNCCL.cpp:1467] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.270985642 ProcessGroupNCCL.cpp:1517] [PG ID 4 PG GUID 35 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:44:12.276425276 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.276436734 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 5] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.276490191 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:44:12.276494561 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 5] Destroy complete.
[rank6]:[I822 23:44:12.282137183 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.282146960 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 6] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:44:12.282192129 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:44:12.282196191 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 6] Destroy complete.
[rank7]:[I822 23:44:12.286300663 ProcessGroupNCCL.cpp:1413] [PG ID 5 PG GUID 37 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:44:12.286311424 ProcessGroupNCCL.cpp:1440] [PG ID 5 PG GUID 37 Rank 7] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:44:12.286367308 ProcessGroupNCCL.cpp:1454] [PG ID 5 PG GUID 37 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.286372186 ProcessGroupNCCL.cpp:1462] [PG ID 5 PG GUID 37 Rank 7] Destroy complete.
[rank0]:[I822 23:44:12.311717696 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:44:12.311736514 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:44:12.311754631 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:44:12.311759046 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.311804415 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.311809536 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 0] Destroy complete.
[rank0]:[I822 23:44:12.311823815 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL destructor entered.
[rank0]:[I822 23:44:12.311839246 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.324643300 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:44:12.324668888 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.324674505 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.324692542 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.324692906 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.324696521 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 1] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.324713058 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.324721170 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 2] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.324755766 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:44:12.324763573 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 1] Destroy complete.
[rank3]:[I822 23:44:12.324761528 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.324767562 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.324771705 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 2] Destroy complete.
[rank3]:[I822 23:44:12.324780300 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank1]:[I822 23:44:12.324782686 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL destructor entered.
[rank2]:[I822 23:44:12.324788655 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL destructor entered.
[rank1]:[I822 23:44:12.324795664 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:12.324802833 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:12.324806506 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 3] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.324805657 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:12.324859812 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:12.324865958 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 3] Destroy complete.
[rank3]:[I822 23:44:12.324882742 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL destructor entered.
[rank3]:[I822 23:44:12.324903532 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.334826790 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:44:12.334849546 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:44:12.334857150 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL destructor entered.
[rank4]:[I822 23:44:12.334879402 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 4] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.334881206 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.334883906 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 4] Operations flushed, joining watchdog thread.
[rank5]:[I822 23:44:12.334906463 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:12.334909891 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 5] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:12.334944066 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:44:12.334954370 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 4] Destroy complete.
[rank5]:[I822 23:44:12.334954158 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:44:12.334958157 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 5] Destroy complete.
[rank4]:[I822 23:44:12.334974562 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:44:12.334976443 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL destructor entered.
[rank5]:[I822 23:44:12.334996960 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:12.334996257 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:44:12.344916589 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:44:12.344942442 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank6]:[I822 23:44:12.344962981 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:12.344966109 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 6] Operations flushed, joining watchdog thread.
[rank7]:[I822 23:44:12.344967087 ProcessGroupNCCL.cpp:1467] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.344987902 ProcessGroupNCCL.cpp:1517] [PG ID 5 PG GUID 37 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:44:12.345031156 ProcessGroupNCCL.cpp:1413] [PG ID 1 PG GUID 1 Rank 7] Starting to destroy process group, flushing operations.
[rank7]:[I822 23:44:12.345039111 ProcessGroupNCCL.cpp:1440] [PG ID 1 PG GUID 1 Rank 7] Operations flushed, joining watchdog thread.
[rank6]:[I822 23:44:12.345038212 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank6]:[I822 23:44:12.345043728 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 6] Destroy complete.
[rank6]:[I822 23:44:12.345060196 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:44:12.345074909 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:44:12.345087758 ProcessGroupNCCL.cpp:1454] [PG ID 1 PG GUID 1 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:12.345092264 ProcessGroupNCCL.cpp:1462] [PG ID 1 PG GUID 1 Rank 7] Destroy complete.
[rank7]:[I822 23:44:12.345112749 ProcessGroupNCCL.cpp:1467] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL destructor entered.
[rank7]:[I822 23:44:12.345129343 ProcessGroupNCCL.cpp:1517] [PG ID 1 PG GUID 1 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
[rank0]:[I822 23:44:12.366195779 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 0] Starting to destroy process group, flushing operations.
[rank0]:[I822 23:44:12.366204979 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 0] Operations flushed, joining watchdog thread.
[rank0]:[I822 23:44:12.366252746 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 0] Watchdog joined, destroying NCCL communicators.
[rank0]:[I822 23:44:12.366257975 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 0] Destroy complete.
[I822 23:44:12.366294534 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL destructor entered.
[I822 23:44:12.366310450 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 0] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:44:12.366358233 TCPStoreLibUvBackend.cpp:130] [c10d - debug] Read callback failed. code:-4095 name:EOF desc:end of file
[I822 23:44:12.366433894 TCPStoreLibUvBackend.cpp:1105] [c10d - debug] Store exit requested

[I822 23:44:12.366441238 TCPStoreLibUvBackend.cpp:1181] [c10d - debug] UV main loop done: res:1
[I822 23:44:12.366444148 TCPStoreLibUvBackend.cpp:1187] [c10d - debug] Walking live handles prior to closing clients
[I822 23:44:12.366447719 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366450413 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366452306 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366454460 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366456544 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366458596 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366460602 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366463134 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:1 is-closing:0
[I822 23:44:12.366534820 TCPStoreLibUvBackend.cpp:1197] [c10d - debug] Walking live handles after closing clients
[I822 23:44:12.366539460 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366541595 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366543510 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366545443 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366547308 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366549339 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366551500 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366553398 TCPStoreLibUvBackend.cpp:1168] [c10d - debug] UV live handle type 12 active:0 is-closing:1
[I822 23:44:12.366555790 TCPStoreLibUvBackend.cpp:1206] [c10d] uv_loop_close failed with:-16 errn:EBUSY desc:resource busy or locked
[I822 23:44:12.366584253 TCPStoreLibUvBackend.cpp:1216] [c10d] uv_loop cleanup finished.
[rank1]:[I822 23:44:12.376280024 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 1] Starting to destroy process group, flushing operations.
[rank1]:[I822 23:44:12.376294859 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 1] Operations flushed, joining watchdog thread.
[rank1]:[I822 23:44:12.376353390 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 1] Watchdog joined, destroying NCCL communicators.
[rank1]:[I822 23:44:12.376357969 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 1] Destroy complete.
[I822 23:44:12.376405349 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL destructor entered.
[I822 23:44:12.376424987 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 1] ProcessGroupNCCL heart beat monitor thread joined.
[rank2]:[I822 23:44:12.395976879 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 2] Starting to destroy process group, flushing operations.
[rank2]:[I822 23:44:12.395990719 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 2] Operations flushed, joining watchdog thread.
[rank2]:[I822 23:44:12.396044206 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 2] Watchdog joined, destroying NCCL communicators.
[rank2]:[I822 23:44:12.396049223 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 2] Destroy complete.
[I822 23:44:12.396087502 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL destructor entered.
[I822 23:44:12.396109669 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 2] ProcessGroupNCCL heart beat monitor thread joined.
[rank3]:[I822 23:44:13.405690023 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 3] Starting to destroy process group, flushing operations.
[rank3]:[I822 23:44:13.405705314 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 3] Operations flushed, joining watchdog thread.
[rank3]:[I822 23:44:13.405766746 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 3] Watchdog joined, destroying NCCL communicators.
[rank3]:[I822 23:44:13.405771172 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 3] Destroy complete.
[I822 23:44:13.405817163 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL destructor entered.
[I822 23:44:13.405837887 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 3] ProcessGroupNCCL heart beat monitor thread joined.
[rank4]:[I822 23:44:13.406142359 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 4] Starting to destroy process group, flushing operations.
[rank4]:[I822 23:44:13.406152329 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 4] Operations flushed, joining watchdog thread.
[rank4]:[I822 23:44:13.406211923 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 4] Watchdog joined, destroying NCCL communicators.
[rank4]:[I822 23:44:13.406217075 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 4] Destroy complete.
[rank5]:[I822 23:44:13.406227447 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 5] Starting to destroy process group, flushing operations.
[rank5]:[I822 23:44:13.406241840 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 5] Operations flushed, joining watchdog thread.
[I822 23:44:13.406263233 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL destructor entered.
[rank6]:[I822 23:44:13.406266999 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 6] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:13.406275323 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 6] Operations flushed, joining watchdog thread.
[I822 23:44:13.406285781 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 4] ProcessGroupNCCL heart beat monitor thread joined.
[rank5]:[I822 23:44:13.406298227 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 5] Watchdog joined, destroying NCCL communicators.
[rank5]:[I822 23:44:13.406304108 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 5] Destroy complete.
[rank6]:[I822 23:44:13.406329229 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 6] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:13.406330737 ProcessGroupNCCL.cpp:1413] [PG ID 0 PG GUID 0 Rank 7] Starting to destroy process group, flushing operations.
[rank6]:[I822 23:44:13.406338201 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 6] Destroy complete.
[rank7]:[I822 23:44:13.406339964 ProcessGroupNCCL.cpp:1440] [PG ID 0 PG GUID 0 Rank 7] Operations flushed, joining watchdog thread.
[I822 23:44:13.406349601 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL destructor entered.
[I822 23:44:13.406367604 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 5] ProcessGroupNCCL heart beat monitor thread joined.
[I822 23:44:13.406379574 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL destructor entered.
[I822 23:44:13.406400100 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 6] ProcessGroupNCCL heart beat monitor thread joined.
[rank7]:[I822 23:44:13.406407029 ProcessGroupNCCL.cpp:1454] [PG ID 0 PG GUID 0 Rank 7] Watchdog joined, destroying NCCL communicators.
[rank7]:[I822 23:44:13.406410351 ProcessGroupNCCL.cpp:1462] [PG ID 0 PG GUID 0 Rank 7] Destroy complete.
[I822 23:44:13.406454874 ProcessGroupNCCL.cpp:1467] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL destructor entered.
[I822 23:44:13.406477230 ProcessGroupNCCL.cpp:1517] [PG ID 0 PG GUID 0 Rank 7] ProcessGroupNCCL heart beat monitor thread joined.
|     Task      |Version|     Metric     |Value |   |Stderr|
|---------------|------:|----------------|-----:|---|-----:|
|all            |       |extractive_match|0.5333|±  |0.0926|
|custom:aime24:0|      1|extractive_match|0.5333|±  |0.0926|

Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 60.43ba/s]
+ set +x
---- 2025-08-22T23:44:20+00:00 RUN END ----
---- 2025-08-22T23:45:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:45:32.463841707 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:46:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:46:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:46:53.085844138 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:47:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:48:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:48:30.064500645 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:49:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:50:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:50:29.003593579 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:51:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:53:01+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:53:03.152577285 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:53:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-22T23:56:58+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I822 23:57:00.074286358 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-22 23:57:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:03:13+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:03:15.151624703 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:03:53 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:09:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:09:39.224817599 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:10:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:15:57+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:15:59.144752332 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:16:38 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:22:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:22:12.126544847 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:22:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:28:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:28:22.167050929 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:29:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:34:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:34:39.122371299 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:35:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:40:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:40:55.753878923 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:41:34 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:47:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:47:11.256869252 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:47:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:53:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:53:32.101842564 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 00:54:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T00:59:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 00:59:50.204193770 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:00:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:05:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:06:01.188150451 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:06:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:12:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:12:11.142925125 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:12:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:18:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:18:29.135251723 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:19:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:24:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:24:50.158679225 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:25:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:31:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:31:04.136972608 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:31:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:37:22+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:37:24.137068636 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:38:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:43:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:43:48.407285760 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:44:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:50:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:50:04.393468437 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:50:43 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T01:56:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 01:56:25.129196479 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 01:57:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:02:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:02:45.311493586 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:03:24 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:09:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:09:07.134674948 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:09:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:15:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:15:22.050966247 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:16:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:21:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:21:32.072340643 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:22:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:27:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:27:47.101450734 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:28:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:34:01+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:34:03.119961425 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:34:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:40:22+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:40:24.058541875 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:41:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:46:33+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:46:35.275233422 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:47:14 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:52:45+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:52:47.197847819 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:53:25 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T02:59:04+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 02:59:06.153182927 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 02:59:44 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:05:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:05:23.131590435 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:06:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:11:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:11:40.137421885 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:12:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:17:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:17:58.229267898 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:18:37 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:24:09+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:24:11.172411256 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:24:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:30:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:30:21.253801270 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:31:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:36:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:36:41.131138660 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:37:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:42:51+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:42:53.038044874 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:43:31 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:49:03+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:49:05.158968542 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:49:43 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T03:55:17+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 03:55:19.188275039 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 03:55:57 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:01:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:01:39.153897508 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:02:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:07:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:08:01.199332701 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:08:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:14:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:14:21.040978410 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:14:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:20:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:20:40.101567946 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:21:18 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:26:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:26:50.112788757 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:27:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:32:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:33:01.212821172 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:33:40 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:39:20+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:39:22.095379127 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:40:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:45:43+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:45:45.122093933 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:46:23 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:51:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:51:57.152837963 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:52:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T04:58:06+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 04:58:08.222738559 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 04:58:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:04:16+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:04:18.193634407 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:04:57 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:10:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:10:30.041385462 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:11:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:16:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:16:51.219789014 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:17:30 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:23:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:23:12.242366353 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:23:51 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:29:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:29:25.257780810 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:30:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:35:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:35:39.149485818 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:36:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:41:55+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:41:57.145659208 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:42:35 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:48:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:48:13.149295830 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:48:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T05:54:24+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 05:54:26.144556806 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 05:55:04 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:00:39+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:00:41.184287083 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:01:20 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:06:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:06:54.189947897 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:07:32 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:13:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:13:10.205825472 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:13:49 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:19:18+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:19:20.103563648 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:19:58 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:25:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:25:30.143151463 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:26:09 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:31:38+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:31:40.225625462 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:32:19 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:37:56+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:37:58.165698311 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:38:36 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:44:10+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:44:12.162799898 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:44:50 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:50:21+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:50:23.189895244 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:51:02 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T06:56:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 06:56:39.149134918 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 06:57:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:02:59+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:03:01.069347986 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:03:39 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:09:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:09:21.116634348 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:09:59 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:15:35+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:15:37.211463246 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:16:15 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:21:52+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:21:54.086212625 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:22:33 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:28:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:28:09.095925003 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:28:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:34:19+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:34:21.187681322 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:35:00 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:40:36+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:40:38.083815361 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:45:01 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:50:41+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:50:43.909293220 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:51:21 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:52:04+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:52:06.199404202 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:52:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:53:37+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:53:39.083788156 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:54:17 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:55:27+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:55:29.105138674 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:56:07 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T07:58:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 07:58:07.174288391 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 07:58:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:02:00+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:02:02.226288549 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:02:41 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:08:11+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:08:13.189337721 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:08:52 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:14:29+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:14:31.305367021 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:15:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:20:47+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:20:49.003712940 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:21:27 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:27:08+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:27:10.135111442 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:27:48 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:33:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:33:30.975883103 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:34:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:39:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:39:48.051123895 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:40:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:46:05+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:46:07.084957862 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:46:45 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:52:17+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:52:19.997104052 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:52:57 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T08:58:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 08:58:32.964835374 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 08:59:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:04:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:04:51.963496331 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:05:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:11:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:11:04.017933977 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:11:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:17:23+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:17:25.927177977 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:18:03 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:23:32+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:23:34.008164977 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:24:12 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:29:48+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:29:50.046317354 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:30:28 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:36:07+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:36:09.062923463 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:36:47 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:42:28+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:42:30.107540963 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:43:08 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:48:46+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:48:48.104361929 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:49:26 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T09:55:02+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 09:55:04.149394912 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 09:55:42 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:01:14+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:01:16.091667631 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:01:54 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:07:30+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:07:32.919276513 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:08:10 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
---- 2025-08-23T10:13:49+00:00 RUN START ----
+ python /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/main.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --task 'custom|aime24|0|0' --temperature 0.0 --top_p 0.6 --output_dir /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/outputs --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt 'You are a helpful assistant.' --use_chat_template --dtype bfloat16 --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1 --seed 0
[I823 10:13:51.933108719 debug.cpp:50] [c10d] The debug level is set to INFO.
INFO 08-23 10:14:29 [__init__.py:244] Automatically detected platform cuda.
usage: main.py [-h] --model MODEL [--dtype DTYPE]
               [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
               [--max-model-length MAX_MODEL_LENGTH]
               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
               [--max-num-seqs MAX_NUM_SEQS]
               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
               [--extra-vllm-args EXTRA_VLLM_ARGS] [--temps TEMPS]
               [--top-ps TOP_PS] [--seeds SEEDS]
               [--max-new-tokens MAX_NEW_TOKENS] --tasks TASKS
               [--batch-size BATCH_SIZE] [--limit-per-task LIMIT_PER_TASK]
               [--system-prompt SYSTEM_PROMPT] [--output_dir OUTPUT_DIR]
main.py: error: unrecognized arguments: --temperature 0.0 --top_p 0.6 --max_new_tokens 32768 --max_model_length 34816 --custom_tasks_directory /code-fsx/yibiaoy-sandbox/SoberReasoningPlus/lighteval_tasks.py --system_prompt You are a helpful assistant. --use_chat_template --max_num_seqs 256 --max_num_batched_tokens 262144 --tensor_parallel_size 8 --pipeline_parallel_size 1 --data_parallel_size 1
